test_name: photoz-wg
#these are the point prediction tests
point:
    #which photo-z predictions do we want to test
    predictions: [MEDIAN_Z, MODE_Z, MEAN_Z]
    
    #what is the true redshift that we will compare with?
    truths: Z_SPEC
    
    #shall we use weights when calculating metrics, if so specify here.
    weights: weights_valid

    #what metrics do we want to measure. "numpy.std" is the standard deviation from numpy
    # and "bh_photo_z_validation.sigma_68" is the sigma_68 metric found in the bh_photo_z_validation.py file
    metrics: [numpy.median, bh_photo_z_validation.sigma_68, bh_photo_z_validation.outlier_fraction]
    
    #do we want to assign an accetable tolerance to each of these tests?
    tolerance:
    
    #Finally do we want to also measure the metrics in some "bins". 
    #we define the column_name: 'string of bins / string of function that makes bins'
    bins: [MEDIAN_Z: '[0.0, 0.3, 0.6, 0.9, 1.3, 2.0]']

    #Should we calculate errors on each metric? if yes state how
    #you can include as many different error functions as you like.
    error_function: [bh_photo_z_validation.bootstrap_mean_error]
    
#these are the pdf tests
pdf: 
    #we can examine individual redshift pdfs. Remove this part you don't need to compare

    #or shall we compare against stacked pdfs
    stacks:
        truths: Z_SPEC

        #which additional bins shall we use to calculate metrics?
        bins: [Z_SPEC: '[0.0, 0.3, 0.6, 0.9, 1.3, 2.0]']
        metrics: [bh_photo_z_validation.ks_test, bh_photo_z_validation.npoisson, bh_photo_z_validation.log_loss, numpy.mean]

        #shall we use weights when calculating metrics, if so specify here.
        weights: weights_valid

