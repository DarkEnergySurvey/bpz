
#add paths to data
trainDataPath: /Users/hoyleb/Documents/python/modules/photoz-wg/simulations/.fits

#if blank use cross validation, else use this.
validDataPath:

#ID column. This is required
ID: ID

#add paths to feature list
featurepath: /Users/hoyleb/Documents/python/modules/photoz-wg/simulations/feat.txt

machineLearningArchitecture: [sklearn.ensemble.RandomForestRegressor, sklearn.ensemble.AdaBoostRegressor]

#machineLearningArchitecture: [sklearn.ensemble.RandomForestClassifier, sklearn.ensemble.AdaBoostClassifier]

#machineLearningArchitecture: [sklearn.neighbors.KNeighborsRegressor,sklearn.linear_model.Lasso,sklearn.linear_model.SGDRegressor, sklearn.linear_model.LassoLars, sklearn.linear_model.Ridge, sklearn.linear_model.LinearRegression,sklearn.ensemble.RandomForestRegressor, sklearn.ensemble.AdaBoostRegressor,sklearn.ensemble.GradientBoostingRegressor]

#machineLearningArchitecture: [sklearn.linear_model.SGDClassifier,sklearn.neighbors.KNeighborsClassifier, sklearn.linear_model.RidgeClassifier,sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis, sklearn.discriminant_analysis.LinearDiscriminantAnalysis,sklearn.tree.DecisionTreeClassifier,sklearn.ensemble.RandomForestClassifier, sklearn.ensemble.ExtraTreesClassifier, sklearn.ensemble.AdaBoostClassifier, sklearn.ensemble.GradientBoostingClassifier,sklearn.ensemble.BaggingClassifier]

#base_estimator:  sklearn.tree.DecisionTreeClassifier
base_estimator: sklearn.tree.DecisionTreeRegressor

#your outputs will be stored here. Classifier and relevant cross validation info
outPutFileBase: /Users/hoyleb/Documents/python/modules/photoz-wg/simulations/outxv20.p

#do you want one training round with hyper-parameters drawn from this pickle file?
hyper_parameter_file:

#parameters, e.g. ml_params.getRandomParams ml_params.getFastParams
params: ml_params.getRandomParams
#extra parameters to fix/pass
extra_params:
#if validDataPath empty then populate this cross validation
numberCrossValidation: 20

#preprocess data
preprocess_function: [augment_data.kde_augment]

#should we have a small number of base learners only for the first N rounds?
small_iters_with_low_n_estimators: 4

#how many machines should we make, we save only the best. Empty=1
iterations: 150

#verbose output?
verbose: False

#should we add weights during validation?
valid_weights:

#should we add weights during training?
train_weights:

#number of cores (for adaboost in xval)
n_jobs: 6

#output feature importances: empty = no feature importance
featureImportance: True

#maximimum number of MLAs [useful if standarising how many machines are explored]
maximum_NMLAs: 150

#which score metric e.g. sklearn.metrics.f1_score | sklearn.metrics.r2_score | ml_metrics.log_loss | ml_metrics.Deltaz_sigma_68_sigma_95 <-
#harmonic mean between sigma 68 and sig 95
score_metric: ml_metrics.Deltaz_sigma_68_sigma_95
#score_metric: sklearn.metrics.f1_score
