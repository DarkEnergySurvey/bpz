{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Photo-z validation metrics on Resampled Alhambra/Cosmos Data</h2>\n",
    "\n",
    "This notebook shows how to load a .yaml /.p pickle file from all 100 of the resampled Alhambra/Cosmos validation files.\n",
    "\n",
    "We then determine the error, from the spread of metric values from the 100 RS validation data, and also from the Cosmic Variance analysis.\n",
    "\n",
    "*You will need to change the path and path_to_pickle_output_file  lines in the below cell*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pickle\n",
    "import bz2\n",
    "import bh_photo_z_validation as pval\n",
    "path = '/Users/hoyleb/Documents/python/modules/photoz-wg/validation/'\n",
    "\n",
    "#these are the current best template and ML codes. Can you beat them!\n",
    "#['point_LSS_MLv2.p.bz2', 'point_LSS_TPLv2.p.bz2', 'point_WL_MLv2.p.bz2','point_WL_TPLv2.p.bz2']\n",
    "#'point_Y1_MLv2.p.bz2','point_Y1_TPLv2.p.bz2']\n",
    "\n",
    "path_to_pickle_output_file = path + 'point_LSS_MLv2.p.bz2'\n",
    "\n",
    "path_to_cos_var_pickle_file = path + 'cosmic_variance_data/cosvariance_cosmos_z_metrics.p.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "almost_black = '#262626'\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams.update({'font.size': 32, \n",
    "                     'axes.linewidth': 5,\n",
    "                    'text.color': almost_black,\n",
    "                    'xtick.major.size': 4,\n",
    "                    'ytick.major.size': 4,\n",
    "                    'legend.fancybox': True,\n",
    "                    'figure.dpi': 300,\n",
    "                    'legend.fontsize': 16,\n",
    "                    'legend.framealpha': 0.8,\n",
    "                    'legend.shadow': True,\n",
    "                    'xtick.labelsize': 22,\n",
    "                    'ytick.labelsize': 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Metrics of interest</h3>\n",
    "\n",
    "Let us decide which metrics we want to measure. Edit this list with metrics that we measured from the validation script. Look at the structure of the validation .yaml file to understand this indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#metrics to plot.\n",
    "#'metric_name': [WHICH_Z_COLUMN, WAY_TO_COMPARE_DISTs, METRIC, [,MEASURED_ON]]\n",
    "metrics = {'sigma_68': ['MEAN_Z', 'metrics_diffz','bh_photo_z_validation.sigma_68', 'delta_z'],\n",
    "           'outlier_fraction': ['MEAN_Z', 'metrics_diffz', \n",
    "                                'bh_photo_z_validation.outlier_fraction', 'diff_1pz'],\n",
    "           'outFrac_2sigma68': ['MEAN_Z', 'metrics_diffz', \n",
    "                                'bh_photo_z_validation.outFrac_2sigma68', 'delta_z'],\n",
    "           'outFrac_3sigma68': ['MEAN_Z', 'metrics_diffz', \n",
    "                                'bh_photo_z_validation.outFrac_3sigma68', 'delta_z'],\n",
    "           'median': ['MEAN_Z', 'metrics_diffz', 'numpy.median', 'delta_z'],\n",
    "           'wl_metric': ['Z_MC', 'metrics_z1_z2', 'bh_photo_z_validation.wl_metric']\n",
    "           }\n",
    "\n",
    "#which bining column should we choose?\n",
    "bin_column = 'MEAN_Z'\n",
    "\n",
    "#requirements from science handbook\n",
    "# http://des-docdb.fnal.gov:8080/cgi-bin/RetrieveFile?docid=20&filename=sciReq-9.86.pdf&version=32\n",
    "# http://des-docdb.fnal.gov:8080/cgi-bin/ShowDocument?docid=1719\n",
    "requirements = {'sigma_68':{'value': 0.12, 'error': 0.003},\n",
    "                'wl_metric': {'value': 0.02, 'error': 0.003},\n",
    "                'outFrac_2sigma68': { 'value': 0.1 , 'error': 0.001 },\n",
    "                'outFrac_3sigma68': {'value': 0.015, 'error': 0.00015 },\n",
    "                }\n",
    "\n",
    "#plotting ranges\n",
    "plt_range = {'sigma_68': [0, 0.5],\n",
    "            'wl_metric': [0, 0.1],\n",
    "            'outFrac_2sigma68': [0, 0.3],\n",
    "            'outFrac_3sigma68': [0, 0.3],\n",
    "             'median' : [-0.2, 0.2],\n",
    "             'outlier_fraction': [0,70]\n",
    "            }\n",
    "\n",
    "metric_latex = {\n",
    "    'sigma_68' :'$\\sigma_{68}(z_{true} - z_{pred})$',\n",
    "    'wl_metric': '$|<dNdz_{true}> - <dNdz_{pred}>|$',\n",
    "    'outFrac_2sigma68': '$f(>2*\\sigma_{68})$',\n",
    "    'outFrac_3sigma68': '$f(>3*\\sigma_{68})$',\n",
    "    'median' : '$\\mu(z_{true} - z_{pred})$',\n",
    "    'outlier_fraction': '$f(|(z_{true} - z_{pred})/(1+z_{true})|>1.5)$'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load results files</h3>\n",
    "\n",
    "Now let's load the .p [pickle] file output from the validation pipeline, and also the cosmic variance results we calculated elsewhere [Ask Youngsoo/Ben/Markus for details].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if path_to_pickle_output_file[-4:] == '.bz2':\n",
    "    res = pickle.load(bz2.BZ2File(path_to_pickle_output_file, 'r'))\n",
    "else:\n",
    "    res = pickle.load(open(path_to_pickle_output_file, 'r'))\n",
    "\n",
    "cosvar = pickle.load(bz2.BZ2File(path_to_cos_var_pickle_file, 'r'))\n",
    "\n",
    "test_dict = cosvar[0]['test']\n",
    "\n",
    "#get the original bins in the test .yaml file\n",
    "bins = np.array(eval(test_dict['bins'][0][bin_column]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "files = res.keys()\n",
    "def get_val(dct, ky):\n",
    "    return dct[ky]\n",
    "\n",
    "sample = files[0].split('/')[-1].split('ScienceCase')[0].split('_.')[1]\n",
    "\n",
    "code = 'ML'\n",
    "if 'bpz' in files[0]:\n",
    "    code = 'TPL'\n",
    "\n",
    "print (sample + ' ' + code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#get the error component from the resampled Alhambra data\n",
    "m_res = {} #temporary value storage\n",
    "metric_res = {} #this holds the median metric value, and std from the RS samples. Also the binned results\n",
    "\n",
    "for m in metrics.keys(): \n",
    "    met_kys = metrics[m]\n",
    "    m_res[m] ={'VALUES': [], 'BINS_ID': {}}\n",
    "    metric_res[m] = {'VALUES':[], 'BINS_ID': {}}\n",
    "    for f in files:\n",
    "        test_name = res[f].keys()[0]\n",
    "        \n",
    "        for j, ky in enumerate(met_kys):\n",
    "            if j ==0:\n",
    "                dct = get_val(res[f][test_name], ky)\n",
    "            else:\n",
    "                dct = get_val(dct, ky)\n",
    "        \n",
    "        #get the unbinned value\n",
    "        m_res[m]['VALUES'].append(dct['VALUE'])\n",
    "        \n",
    "        #Each resampled file can have a different <z> per bin, so find the \n",
    "        #index of the original bins [in test.yaml] that corresponds to this redshift\n",
    "        for j, zbn in enumerate(dct['bins'][bin_column]['BIN_CENTERS']):\n",
    "            if np.isfinite(zbn) and zbn > 0:\n",
    "                indj = np.where((zbn < bins[1:]) * (zbn >= bins[0:-1]))[0][0]\n",
    "                if indj not in m_res[m]['BINS_ID']:\n",
    "                    m_res[m]['BINS_ID'][indj] = {}\n",
    "                    m_res[m]['BINS_ID'][indj]['BIN_CENTER'] = []\n",
    "                    m_res[m]['BINS_ID'][indj]['VALUE'] = []\n",
    "                \n",
    "                m_res[m]['BINS_ID'][indj]['BIN_CENTER'].append(zbn)\n",
    "                m_res[m]['BINS_ID'][indj]['VALUE'].append(dct['bins'][bin_column]['VALUE'][j])\n",
    "\n",
    "\n",
    "    #get the median value and the error component from the 68% [standard deviation] of the resampled data.\n",
    "    metric_res[m]['SIGMA'] = pval.sigma_68(m_res[m]['VALUES'])\n",
    "    metric_res[m]['VALUES'] = np.median(m_res[m]['VALUES']) \n",
    "\n",
    "    for j, indj in enumerate(m_res[m]['BINS_ID'].keys()):\n",
    "        if len(m_res[m]['BINS_ID'][indj]['BIN_CENTER']) > 80:\n",
    "            metric_res[m]['BINS_ID'][indj] = {}\n",
    "            metric_res[m]['BINS_ID'][indj]['BIN_CENTER'] = np.median(m_res[m]['BINS_ID'][indj]['BIN_CENTER'])\n",
    "        \n",
    "            #overwrite the lists to save some memory space\n",
    "            metric_res[m]['BINS_ID'][indj]['SIGMA'] =  pval.sigma_68(m_res[m]['BINS_ID'][indj]['VALUE'])\n",
    "            metric_res[m]['BINS_ID'][indj]['VALUE'] = np.median(m_res[m]['BINS_ID'][indj]['VALUE'])\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Next add in CosmicVariance error component</h3>\n",
    "\n",
    "We repeat the above, and additionally calculate the error on each metric from cosmic variance. and from validating on high quality photo-z instead of spec-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#this will hold the median metric values, RS errors, AND CosVar Errors\n",
    "sample_var_cosz = {}\n",
    "sample_var_truez = {}\n",
    "\n",
    "for m in metrics.keys(): \n",
    "    met_kys = metrics[m]\n",
    "\n",
    "    sample_var_cosz[m] = {'VALUES':[], 'BINS_ID': {}}\n",
    "    sample_var_truez[m] = {'VALUES':[], 'BINS_ID': {}}\n",
    "    \n",
    "    #we only care about the bin numbers that we have measured data in\n",
    "    for j, indj in enumerate(metric_res[m]['BINS_ID']):\n",
    "        sample_var_truez[m]['BINS_ID'][indj] = {}\n",
    "        sample_var_cosz[m]['BINS_ID'][indj] = {}\n",
    "        \n",
    "        sample_var_truez[m]['BINS_ID'][indj]['VALUE'] = []\n",
    "        sample_var_cosz[m]['BINS_ID'][indj]['VALUE'] = []\n",
    "        \n",
    "        sample_var_truez[m]['BINS_ID'][indj]['BIN_CENTER'] = []\n",
    "        sample_var_cosz[m]['BINS_ID'][indj]['BIN_CENTER'] = []\n",
    "\n",
    "                \n",
    "    #loop over all simulated files.\n",
    "    for f in cosvar:\n",
    "        for j, ky in enumerate(met_kys):\n",
    "            if j ==0:\n",
    "                cos_dct = get_val(f['COSMOS_Z'], ky)\n",
    "                z_dct = get_val(f['Z'], ky)\n",
    "            else:\n",
    "                cos_dct = get_val(cos_dct, ky)\n",
    "                z_dct = get_val(z_dct, ky)\n",
    "\n",
    "        #get the value for all unbinned data        \n",
    "        sample_var_cosz[m]['VALUES'].append(cos_dct['VALUE'])    \n",
    "        sample_var_truez[m]['VALUES'].append(z_dct['VALUE'])\n",
    "  \n",
    "        #determine which bin we are referring to\n",
    "        for j, zbn in enumerate(cos_dct['bins'][bin_column]['BIN_CENTERS']):\n",
    "            indj = np.where((zbn < bins[1:]) * (zbn >= bins[0:-1]))[0][0]\n",
    "            \n",
    "            #if we care about this bin, then save values [and what was the center for this realisiation.]\n",
    "            if indj in sample_var_cosz[m]['BINS_ID'].keys():\n",
    "                sample_var_cosz[m]['BINS_ID'][indj]['BIN_CENTER'].append(cos_dct['bins'][bin_column]['BIN_CENTERS'][j])\n",
    "                sample_var_cosz[m]['BINS_ID'][indj]['VALUE'].append(cos_dct['bins'][bin_column]['VALUE'][j])\n",
    "            \n",
    "        for j, zbn in enumerate(z_dct['bins'][bin_column]['BIN_CENTERS']):  \n",
    "            indj = np.where((zbn < bins[1:]) * (zbn >= bins[0:-1]))[0][0]\n",
    "            \n",
    "            if indj in sample_var_truez[m]['BINS_ID'].keys():\n",
    "                sample_var_truez[m]['BINS_ID'][indj]['BIN_CENTER'].append(z_dct['bins'][bin_column]['BIN_CENTERS'][j])\n",
    "                sample_var_truez[m]['BINS_ID'][indj]['VALUE'].append(z_dct['bins'][bin_column]['VALUE'][j])\n",
    "            \n",
    "    \n",
    "    #calculate the 68% spread of these metric values\n",
    "    sample_var_cosz[m]['SIGMA']  = pval.sigma_68(sample_var_cosz[m]['VALUES'])\n",
    "    sample_var_truez[m]['SIGMA']  = pval.sigma_68(sample_var_truez[m]['VALUES'])\n",
    "\n",
    "    sample_var_cosz[m]['VALUES']  = np.median(sample_var_cosz[m]['VALUES'])\n",
    "    sample_var_truez[m]['VALUES']  = np.median(sample_var_truez[m]['VALUES'])\n",
    "    \n",
    "    #for each bin number, calculate the bin center, sigmas and mean value\n",
    "    for j, indj in enumerate(sample_var_truez[m]['BINS_ID']):\n",
    "        sample_var_truez[m]['BINS_ID'][indj]['BIN_CENTER'] = np.median(sample_var_truez[m]['BINS_ID'][indj]['BIN_CENTER'])\n",
    "        sample_var_truez[m]['BINS_ID'][indj]['SIGMA'] = pval.sigma_68(sample_var_truez[m]['BINS_ID'][indj]['VALUE'])\n",
    "        sample_var_truez[m]['BINS_ID'][indj]['VALUE'] = np.median(sample_var_truez[m]['BINS_ID'][indj]['VALUE'])\n",
    "        \n",
    "    for j, indj in enumerate(sample_var_cosz[m]['BINS_ID']):\n",
    "        sample_var_cosz[m]['BINS_ID'][indj]['BIN_CENTER'] = np.median(sample_var_cosz[m]['BINS_ID'][indj]['BIN_CENTER'])\n",
    "        sample_var_cosz[m]['BINS_ID'][indj]['SIGMA'] = pval.sigma_68(sample_var_cosz[m]['BINS_ID'][indj]['VALUE'])\n",
    "        sample_var_cosz[m]['BINS_ID'][indj]['VALUE'] = np.median(sample_var_cosz[m]['BINS_ID'][indj]['VALUE'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot both error components</h3>\n",
    "\n",
    "Let's add the cosmic variance error in quadrature to the ReSample error, and plot the results for each metric, as a function of the tomographic bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Ms = 8\n",
    "lw = 3\n",
    "print (code + sample)\n",
    "for m in metric_res:\n",
    "    f= plt.figure()\n",
    "    indjs = metric_res[m]['BINS_ID'].keys()\n",
    "    z = np.array([metric_res[m]['BINS_ID'][indj]['BIN_CENTER'] for indj in indjs])\n",
    "    errRS = np.array([metric_res[m]['BINS_ID'][indj]['SIGMA'] for indj in indjs]) #err from ReSample\n",
    "    errCV = np.array([sample_var_cosz[m]['BINS_ID'][indj]['SIGMA'] for indj in indjs]) #err from CosVar to photo-z\n",
    "    errNonSpecZ = np.array([np.abs(sample_var_cosz[m]['BINS_ID'][indj]['SIGMA'] - sample_var_truez[m]['BINS_ID'][indj]['SIGMA']) \n",
    "                             for indj in indjs]) #err from COSMOS photo-z to true-z\n",
    "    \n",
    "    err = np.sqrt(errRS**2 + errCV**2 + errNonSpecZ**2) #add in quadrature\n",
    "    y = np.array([metric_res[m]['BINS_ID'][indj]['VALUE'] for indj in indjs])\n",
    "    \n",
    "    plt.errorbar(np.array(z)-0.02, y, yerr=errNonSpecZ, fmt=',', markersize=str(Ms), elinewidth=lw,label='Photz-Specz err')\n",
    "    plt.errorbar(np.array(z), y, yerr=errCV , fmt=',',markersize=str(Ms), elinewidth=lw, label='CosVar err')\n",
    "    plt.errorbar(np.array(z)-0.01, y, yerr=errRS, fmt=',', markersize=str(Ms), elinewidth=lw,label='RS mag err')\n",
    "    plt.errorbar(np.array(z)+0.01, y, yerr=err, fmt='o',markersize=str(Ms), elinewidth=lw, label='All errs')\n",
    "    \n",
    "    #get requirements:\n",
    "    if m in requirements:\n",
    "        #err_req = [requirements[m]['error']]*len(y)\n",
    "        #plt.errorbar(np.array(z)+0.01, y, yerr=err, fmt='*',markersize=str(Ms), elinewidth=lw, label='Error req.')\n",
    "        \n",
    "        val_req = [requirements[m]['value']]*len(z)\n",
    "        plt.plot(np.array(np.sort(z)), val_req,'--', linewidth=3,label='Value req.')\n",
    "    \n",
    "    plt.xlabel('Tomographic redshift bin')\n",
    "    plt.title(code + ' code on ' + sample)\n",
    "    plt.ylim(plt_range[m])\n",
    "    plt.ylabel(metric_latex[m])\n",
    "    plt.legend(loc=2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(code + sample + m + '.pdf')\n",
    "    \n",
    "    \n",
    "    print ('metric: | {:}'.format(m))\n",
    "    mean = metric_res[m]['VALUES']\n",
    "    rsErr = metric_res[m]['SIGMA']\n",
    "    cosVarErr = sample_var_cosz[m]['SIGMA']\n",
    "    photSpecErr = np.abs(sample_var_cosz[m]['SIGMA'] - sample_var_truez[m]['SIGMA'])\n",
    "    err = np.sqrt(rsErr**2 + cosVarErr**2 + photSpecErr**2)\n",
    "    print ('Mean & ReSamp Err & SampVar Err & PhotSpec Err & Combined Err \\\\\\\\')\n",
    "    print ('{:0.4f} \\pm \\\\bf {:0.4f}  ( {:0.1e} \\pm {:0.1e} \\pm {:0.1e}) \\\\\\\\ '.format(mean,err, rsErr, cosVarErr, photSpecErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generating the sample (cosmic) variance</h3>\n",
    "\n",
    "This code snippet loads in a heap of validation script outputs, measured on each patch in a simulation, and stores the resulting list in the cosmic variance file in the validation directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = glob.glob('/Users/hoyleb/DATA/DES/PHOTOZ/COSMIC_VAR/*.Cosmos*.p')\n",
    "res = []\n",
    "for fil in files:\n",
    "    r = pickle.load(open(fil, 'r'))\n",
    "    res.append(r)\n",
    "print len(res)\n",
    "#pickle.dump(res, bz2.BZ2File(path + '/cosmic_variance_data/cosvariance_cosmos_z_metrics.p.bz2', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
