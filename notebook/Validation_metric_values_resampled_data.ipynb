{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Photo-z validation metrics on Resampled Cosmos Data</h2>\n",
    "\n",
    "This notebook shows how to load a .yaml /.p pickle file from all 200 of the resampled Alhambra/Cosmos validation files.\n",
    "\n",
    "We then determine the error, from the spread of metric values from the 200 RS validation data, and also from the Cosmic Variance analysis.\n",
    "\n",
    "*You will need to change the path and path_to_pickle_output_file  lines in the below cell*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pickle\n",
    "import bz2\n",
    "import copy\n",
    "import bh_photo_z_validation as pval\n",
    "path = '/Users/hoyleb/Documents/python/modules/photoz-wg/validation/'\n",
    "\n",
    "#these are the current best template and ML codes. Can you beat them!\n",
    "#\n",
    "#path_to_pickle_output_file = path + 'WL_CLASS.METACAL.BPZ_HIZ.p.bz2'\n",
    "#path_to_pickle_output_file = path + 'WL_CLASS.METACAL.NEWBPZ.p'\n",
    "path_to_pickle_output_file = path + 'LSS_CLASS.COADD.BPZ_ORIG.p.bz2'\n",
    "path_to_pickle_output_file = path + 'LSS_CLASS.COADD.BPZ_HIZ.p.bz2'\n",
    "path_to_pickle_output_file = path + 'LSS_CLASS.MOF.BPZ_HIZ.p.bz2'\n",
    "path_to_pickle_output_file = path + 'LSS_CLASS.MOF.BPZ_ORIG.p.bz2'\n",
    "path_to_pickle_output_file = path + 'Y1_CLASS.MOF.BPZ_ORIG.p.bz2'\n",
    "path_to_pickle_output_file = path + 'Y1_CLASS.MOF.BPZ_HIZ.p.bz2'\n",
    "\n",
    "\n",
    "#leave this alone\n",
    "path_to_cos_var_pickle_file = path + 'cosmic_variance_data/cosvariance_metrics.cos0alh.p.bz2'\n",
    "\n",
    "if 'Y1' in path_to_pickle_output_file:\n",
    "    science_sample = 'Y1'\n",
    "\n",
    "if 'LSS' in path_to_pickle_output_file:\n",
    "    science_sample = 'LSS'\n",
    "\n",
    "if 'WL' in path_to_pickle_output_file:\n",
    "    science_sample = 'WL'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class scaled_error():\n",
    "    def __init__(self, const, scaling_type):\n",
    "        self.const = const\n",
    "        self.scaling = None\n",
    "        if scaling_type == 'one_plus_z':\n",
    "            self.scaling = self.one_plus_z \n",
    "        else:\n",
    "            self.scaling = self.const_with_z\n",
    "        \n",
    "    def one_plus_z(self, z):\n",
    "        return np.array((1 + z) * self.const)\n",
    "    \n",
    "    def const_with_z(self, z):\n",
    "        return np.array(len(z) * [self.const])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#something break latex plotting there, I don't know what it is.\n",
    "#ignore this cell.\n",
    "1 / 0\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({\n",
    "'lines.linewidth':1.0,\n",
    "'lines.linestyle':'-',\n",
    "'lines.color':'black',\n",
    "'font.family':'serif',\n",
    "'font.weight':'normal',\n",
    "'font.size':10.0,\n",
    "'text.color':'black',\n",
    "'text.usetex':True,\n",
    "'axes.edgecolor':'black',\n",
    "'axes.linewidth':1.0,\n",
    "'axes.grid':False,\n",
    "'axes.titlesize':'x-large',\n",
    "'axes.labelsize':'x-large',\n",
    "'axes.labelweight':'normal',\n",
    "'axes.labelcolor':'black',\n",
    "'axes.formatter.limits':[-4,4],\n",
    "'xtick.major.size':7,\n",
    "'xtick.minor.size':4,\n",
    "'xtick.major.pad':8,\n",
    "'xtick.minor.pad':8,\n",
    "'xtick.labelsize':'x-large',\n",
    "'xtick.minor.width':1.0,\n",
    "'xtick.major.width':1.0,\n",
    "'ytick.major.size':7,\n",
    "'ytick.minor.size':4,\n",
    "'ytick.major.pad':8,\n",
    "'ytick.minor.pad':8,\n",
    "'ytick.labelsize':'x-large',\n",
    "'ytick.minor.width':1.0,\n",
    "'ytick.major.width':1.0,\n",
    "'legend.numpoints':1,\n",
    "'legend.fontsize':'x-large',\n",
    "'legend.shadow':False,\n",
    "'legend.frameon':False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "almost_black = '#262626'\n",
    "plt.rcParams['figure.figsize'] = (24, 8)\n",
    "plt.rcParams.update({'font.size': 12, \n",
    "                     'axes.linewidth': 5,\n",
    "                    'text.color': almost_black,\n",
    "                    'xtick.major.size': 4,\n",
    "                    'ytick.major.size': 4,\n",
    "                    'legend.fancybox': True,\n",
    "                    'figure.dpi': 300,\n",
    "                    'legend.fontsize': 16,\n",
    "                    'legend.framealpha': 0.8,\n",
    "                    'legend.shadow': True,\n",
    "                    'xtick.labelsize': 12,\n",
    "                    'ytick.labelsize': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Set up the requirements and plotting </h3>\n",
    "\n",
    "Define what plotting symbols to use, and also Y5 requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#which bining column should we choose?\n",
    "bin_column = 'MEAN_Z'\n",
    "\n",
    "#requirements from science handbook\n",
    "# http://des-docdb.fnal.gov:8080/cgi-bin/RetrieveFile?docid=20&filename=sciReq-9.86.pdf&version=32\n",
    "# http://des-docdb.fnal.gov:8080/cgi-bin/ShowDocument?docid=1719\n",
    "requirements = {'sigma_68':{'value': scaled_error(0.12, 'const'), 'error': scaled_error(0.003, 'const')},\n",
    "                'wl_metric': {'value': scaled_error( 0.02, 'const'), 'error': scaled_error(0.02, 'const')},\n",
    "                'outFrac_2sigma68': { 'value': scaled_error(0.1, 'one_plus_z') , 'error': scaled_error(0.001, 'const') },\n",
    "                'outFrac_3sigma68': {'value': scaled_error(0.015, 'one_plus_z'), 'error': scaled_error(0.0015, 'const') },\n",
    "                'median': {'value': scaled_error(0, 'one_plus_z'), 'error': scaled_error(0.001, 'one_plus_z') },\n",
    "                'median_1pz': {'value': scaled_error(0, 'one_plus_z'), 'error': scaled_error(0.001, 'one_plus_z') },\n",
    "                }\n",
    "\n",
    "    \n",
    "#plotting ranges\n",
    "plt_range = {'sigma_68': [0, 0.5],\n",
    "            'wl_metric': [0, 0.1],\n",
    "            'outFrac_2sigma68': [0, 0.3],\n",
    "            'outFrac_3sigma68': [0, 0.3],\n",
    "             'median' : [-0.1, 0.1],\n",
    "             'sigma_68_1pz': [0, 0.2],\n",
    "            'median_1pz': [-0.1, 0.1]\n",
    "             \n",
    "            }\n",
    "\n",
    "metric_latex = {\n",
    "    'sigma_68' :'$\\sigma_{68}(z_{true} - z_{pred})$',\n",
    "    'sigma_68_1pz' :'$\\sigma_{68}((z_{true} - z_{pred})/(1 + z_{true}))$',\n",
    "    'wl_metric': '$|<z_{true}> - <z^{MC}_{pred}>|$',\n",
    "    'outFrac_2sigma68': '$f(>2*\\sigma_{68})$',\n",
    "    'outFrac_3sigma68': '$f(>3*\\sigma_{68})$',\n",
    "    'outFrac_2sigma68_1pz': '$f(>2*\\sigma_{68}/(1+z))$',\n",
    "    'outFrac_3sigma68_1pz': '$f(>3*\\sigma_{68}/(1+z))$',\n",
    "    'median' : '$\\mu(z_{true} - z_{pred})$',\n",
    "    'median_1pz' : '$\\mu(z_{true} - z_{pred})/(1 + z_{true})$',\n",
    "    'delta_sigma_crit': '$\\Delta \\Sigma_{C}'\n",
    "}\n",
    "\n",
    "metric_description = {\n",
    "    'sigma_68' :'68% spread (std.) of delta=(z_true - z_phot)',\n",
    "    'sigma_68_1pz' :'68% spread of delta=(z_true - z_phot)/(1 + z_true))',\n",
    "    'wl_metric': '|<z_true> - <z^MC_phot>|$',\n",
    "    'outFrac_2sigma68': 'Frac. data with abs(delta) >2*sigma_68',\n",
    "    'outFrac_3sigma68': 'Frac data with abs(detla) >3*sigma_68',\n",
    "    'outFrac_2sigma68_1pz': 'Frac data with abs(deta/(1+z)) >2*sigma_68',\n",
    "    'outFrac_3sigma68_1pz': 'Frac data with abs(deta/(1+z)) >3*sigma_68',\n",
    "    'median' : '$median(z_true - z_phot)$',\n",
    "    'median_1pz' : '$median (delta/(1+z)$',\n",
    "    'delta_sigma_crit': '$Delta Sigma Critical'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3>Metrics of interest</h3>\n",
    "\n",
    "Let us decide which metrics we want to measure. Edit this list with metrics that we measured from the validation script. Look at the structure of the validation .yaml file to understand this indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if science_sample == 'WL':\n",
    "    weight = ['IN_WL_SAMPLE']\n",
    "    \n",
    "    #'metric_name': [WHICH_Z_COLUMN, METRIC NAME]\n",
    "    metrics = {\n",
    "        'median': ['MEAN_Z', 'vlfn.median'],    \n",
    "        'sigma_68': ['MEAN_Z', 'vlfn.sigma_68'],\n",
    "        'outFrac_2sigma68':['MEAN_Z','vlfn.outFrac_2sigma68'],  \n",
    "        'wl_metric': ['Z_MC', 'vlfn.wl_metric'],\n",
    "        #'delta_sigma_crit': ['Z_MC', 'vlfn.delta_sigma_crit']      \n",
    "           }\n",
    "\n",
    "    #what is the WL binning structure.\n",
    "    bins = [0, 0.1, 0.2, 0.43, 0.63, 0.9, 1.3]\n",
    "\n",
    "    \n",
    "if science_sample == 'LSS':\n",
    "    weight = ['IN_LSS_SAMPLE'] # list of applicable weights\n",
    "    \n",
    "    #lss has other different requirements\n",
    "    requirements = {\n",
    "                'sigma_68':{ 'value': scaled_error(0.03, 'one_plus_z') , 'error': scaled_error(0.03, 'const')},\n",
    "                'outFrac_2sigma68': { 'value': scaled_error(0.1, 'const') , 'error': scaled_error(0.001, 'const') },\n",
    "                'outFrac_3sigma68': {'value': scaled_error(0.015, 'const'), 'error': scaled_error(0.0015, 'const') },\n",
    "                'median': {'value': scaled_error(0, 'one_plus_z'), 'error': scaled_error(0.001, 'one_plus_z') },\n",
    "                'median_1pz': {'value': scaled_error(0, 'const_with_z'), 'error': scaled_error(0.001, 'one_plus_z') },\n",
    "                'sigma_68_1pz':{ 'value': scaled_error(0.03, 'const_with_z') , 'error': scaled_error(0.03, 'const')},\n",
    "             }\n",
    "          \n",
    "    #'metric_name': [WHICH_Z_COLUMN, METRIC NAME]\n",
    "    metrics = {\n",
    "        'median': ['MEAN_Z', 'vlfn.median'],\n",
    "        'median_1pz': ['MEAN_Z', 'vlfn.median_1pz'],           \n",
    "        'sigma_68': ['MEAN_Z', 'vlfn.sigma_68'],\n",
    "        'sigma_68_1pz': ['MEAN_Z','vlfn.sigma_68_1pz'], \n",
    "        'outFrac_2sigma68':['MEAN_Z','vlfn.outFrac_2sigma68'], \n",
    "        'outFrac_3sigma68':['MEAN_Z','vlfn.outFrac_3sigma68']     \n",
    "       }\n",
    "    \n",
    "    #plotting ranges\n",
    "    plt_range = {\n",
    "        'sigma_68': [0, 0.2],\n",
    "        'sigma_68_1pz': [0, 0.1],\n",
    "        'outFrac_2sigma68': [0, 0.15],\n",
    "        'outFrac_3sigma68': [0, 0.1],\n",
    "        'median' : [-0.05, 0.05],\n",
    "        'median_1pz': [-0.02, 0.02]\n",
    "        \n",
    "            }\n",
    "    #what is the WL binning structure.\n",
    "    bins = [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "if science_sample == 'Y1':\n",
    "    weight = ['IN_Y1_SAMPLE']\n",
    "    bins = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0,1.2,1.4]\n",
    "    \n",
    "    metrics = {\n",
    "        'median': ['MEAN_Z', 'vlfn.median'],\n",
    "        'median_1pz': ['MEAN_Z', 'vlfn.median_1pz'],           \n",
    "        'sigma_68': ['MEAN_Z', 'vlfn.sigma_68'],\n",
    "        'sigma_68_1pz': ['MEAN_Z','vlfn.sigma_68_1pz'], \n",
    "        'outFrac_2sigma68':['MEAN_Z','vlfn.outFrac_2sigma68'], \n",
    "        'outFrac_3sigma68':['MEAN_Z','vlfn.outFrac_3sigma68']     \n",
    "       }\n",
    "    \n",
    "    \n",
    "#generate \"key\" for cosmic variance data structure\n",
    "bns_str = ','.join(str(bi) for bi in bins).replace(' ','')\n",
    "print bns_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load results files</h3>\n",
    "\n",
    "Now let's load the .p [pickle] file output from the validation pipeline, and also the cosmic variance results we calculated elsewhere [Ask Youngsoo/Ben/Markus for details].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if path_to_pickle_output_file[-4:] == '.bz2':\n",
    "    res = pickle.load(bz2.BZ2File(path_to_pickle_output_file, 'r'))\n",
    "else:\n",
    "    res = pickle.load(open(path_to_pickle_output_file, 'r'))\n",
    "\n",
    "original_test_config = copy.copy(res['test_config'])\n",
    "del res['test_config']\n",
    "\n",
    "cosvar = pickle.load(bz2.BZ2File(path_to_cos_var_pickle_file, 'r'))\n",
    "\n",
    "test_dict = cosvar[0][bns_str]['test']\n",
    "\n",
    "print (test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "files = res.keys()\n",
    "def get_val(dct, ky):\n",
    "    return dct[ky]\n",
    "label = path_to_pickle_output_file.split('/')[-1].replace('.p', '')\n",
    "print (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#get the error component from the 200 SLR corrections applied to the validation files\n",
    "    \n",
    "m_res = {} #temporary value storage\n",
    "metric_res = {} #this holds the median metric value, and std from the RS samples. Also the binned results\n",
    "\n",
    "for m in metrics.keys(): \n",
    "    met_kys = metrics[m]\n",
    "    m_res[m] = {}\n",
    "    metric_res[m] = {}\n",
    "\n",
    "    for wgt in weight:\n",
    "        m_res[m][wgt] = {'VALUES': [], 'BINS_ID': {}}\n",
    "        metric_res[m][wgt] = {'VALUES': [], 'BINS_ID': {}}\n",
    "            \n",
    "    for f in files:\n",
    "        for j, ky in enumerate(met_kys):\n",
    "            if j ==0:\n",
    "                dct_ = get_val(res[f], ky)\n",
    "            else:\n",
    "                dct_ = get_val(dct_, ky)\n",
    "        #now get the results as weighte by each\n",
    "        for wgt in weight:\n",
    "            dct = dct_[wgt]\n",
    "            #get the unbinned value\n",
    "            m_res[m][wgt]['VALUES'].append(dct['value'])\n",
    "        \n",
    "            #Each resampled file can have a different <z> per bin, so find the \n",
    "            #index of the original bins [in test.yaml] that corresponds to this redshift\n",
    "            for j, zbn in enumerate(dct['bins'][bin_column]['bin_center']):\n",
    "                if np.isfinite(zbn) and zbn > 0:\n",
    "                    indj = np.where((zbn < bins[1:]) * (zbn >= bins[0:-1]))[0][0]\n",
    "                    if indj not in m_res[m][wgt]['BINS_ID']:\n",
    "                        m_res[m][wgt]['BINS_ID'][indj] = {}\n",
    "                        m_res[m][wgt]['BINS_ID'][indj]['BIN_CENTER'] = []\n",
    "                        m_res[m][wgt]['BINS_ID'][indj]['VALUE'] = []\n",
    "\n",
    "                    m_res[m][wgt]['BINS_ID'][indj]['BIN_CENTER'].append(zbn)\n",
    "                    m_res[m][wgt]['BINS_ID'][indj]['VALUE'].append(dct['bins'][bin_column]['value'][j])\n",
    "\n",
    "\n",
    "            #get the median value and the error component from the 68% [standard deviation] of the resampled data.\n",
    "            metric_res[m][wgt]['SIGMA'] = pval.sigma_68(m_res[m][wgt]['VALUES'])\n",
    "            metric_res[m][wgt]['VALUES'] = np.median(m_res[m][wgt]['VALUES']) \n",
    "\n",
    "            for j, indj in enumerate(m_res[m][wgt]['BINS_ID'].keys()):\n",
    "\n",
    "                    metric_res[m][wgt]['BINS_ID'][indj] = {}\n",
    "                    metric_res[m][wgt]['BINS_ID'][indj]['BIN_CENTER'] = np.median(m_res[m][wgt]['BINS_ID'][indj]['BIN_CENTER'])\n",
    "\n",
    "                    #overwrite the lists to save some memory space\n",
    "                    metric_res[m][wgt]['BINS_ID'][indj]['SIGMA'] =  pval.sigma_68(m_res[m][wgt]['BINS_ID'][indj]['VALUE'])\n",
    "                    metric_res[m][wgt]['BINS_ID'][indj]['VALUE'] = np.median(m_res[m][wgt]['BINS_ID'][indj]['VALUE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Next add in CosmicVariance error component</h3>\n",
    "\n",
    "We repeat the above, and additionally calculate the error on each metric from cosmic variance. and from validating on high quality photo-z instead of spec-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#this will hold the median metric values, RS errors, AND CosVar Errors\n",
    "sample_var_cosz = {}\n",
    "sample_var_truez = {}\n",
    "\n",
    "for m in metrics.keys(): \n",
    "    met_kys = metrics[m]\n",
    "    \n",
    "    sample_var_truez[m] = {}\n",
    "    for wgt in weight:\n",
    "        sample_var_truez[m][wgt] = {'VALUES': [], 'BINS_ID': {}}\n",
    "        \n",
    "    for wgt in weight:\n",
    "\n",
    "        #we only care about the bin numbers that we have measured data in\n",
    "        for j, indj in enumerate(metric_res[m][wgt]['BINS_ID']):\n",
    "            sample_var_truez[m][wgt]['BINS_ID'][indj] = {}\n",
    "            sample_var_truez[m][wgt]['BINS_ID'][indj]['VALUE'] = []\n",
    "            sample_var_truez[m][wgt]['BINS_ID'][indj]['BIN_CENTER'] = []\n",
    "\n",
    "\n",
    "        #loop over all simulated files.\n",
    "        for f in cosvar:\n",
    "            for j, ky in enumerate(met_kys):\n",
    "                if j ==0:\n",
    "                    z_dct = get_val(f[bns_str]['Z'], ky)\n",
    "                else:\n",
    "                    z_dct = get_val(z_dct, ky)\n",
    "            \n",
    "            #slight massaging of weight from simulation data\n",
    "            tst_weight = copy.copy(wgt)\n",
    "            if tst_weight not in z_dct:\n",
    "                if 'WL' in tst_weight or 'ZLENS' in tst_weight:\n",
    "                    tst_weight = 'IN_WL_SAMPLE'\n",
    "                else:\n",
    "                    tst_weight = z_dct.keys()[0]\n",
    "            \n",
    "            z_dct = z_dct[tst_weight]\n",
    "            #get the value for all unbinned data        \n",
    "            sample_var_truez[m][wgt]['VALUES'].append(z_dct['value'])\n",
    "\n",
    "            #determine which bin we are referring to\n",
    "\n",
    "            for j, zbn in enumerate(z_dct['bins'][bin_column]['bin_center']):  \n",
    "                indj = np.where((zbn < bins[1:]) * (zbn >= bins[0:-1]))[0][0]\n",
    "                if indj in sample_var_truez[m][wgt]['BINS_ID'].keys():\n",
    "                    sample_var_truez[m][wgt]['BINS_ID'][indj]['BIN_CENTER'].append(z_dct['bins'][bin_column]['bin_center'][j])\n",
    "                    sample_var_truez[m][wgt]['BINS_ID'][indj]['VALUE'].append(z_dct['bins'][bin_column]['value'][j])\n",
    "\n",
    "\n",
    "        #calculate the 68% spread of these metric values\n",
    "        sample_var_truez[m][wgt]['SIGMA']  = pval.sigma_68(sample_var_truez[m][wgt]['VALUES'])\n",
    "        sample_var_truez[m][wgt]['VALUES']  = np.median(sample_var_truez[m][wgt]['VALUES'])\n",
    "\n",
    "        #for each bin number, calculate the bin center, sigmas and mean value\n",
    "        for j, indj in enumerate(sample_var_truez[m][wgt]['BINS_ID']):\n",
    "            sample_var_truez[m][wgt]['BINS_ID'][indj]['BIN_CENTER'] = np.median(\n",
    "                sample_var_truez[m][wgt]['BINS_ID'][indj]['BIN_CENTER']\n",
    "            )\n",
    "            sample_var_truez[m][wgt]['BINS_ID'][indj]['SIGMA'] = pval.sigma_68(\n",
    "                sample_var_truez[m][wgt]['BINS_ID'][indj]['VALUE']\n",
    "                )\n",
    "            sample_var_truez[m][wgt]['BINS_ID'][indj]['VALUE'] = np.median(\n",
    "                sample_var_truez[m][wgt]['BINS_ID'][indj]['VALUE']\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plot both error components</h3>\n",
    "\n",
    "Let's add the cosmic variance error in quadrature to the ReSample error, and plot the results for each metric, as a function of the tomographic bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Ms = 12\n",
    "lw = 4\n",
    "show_z = True\n",
    "#for each of the metrics\n",
    "for m in metric_res:\n",
    "    \n",
    "    #for each of the weights\n",
    "    for wght in metric_res[m]:\n",
    "        f= plt.figure()\n",
    "        indjs = metric_res[m][wght]['BINS_ID'].keys()\n",
    "        z = np.array([metric_res[m][wght]['BINS_ID'][indj]['BIN_CENTER'] for indj in indjs])\n",
    "        errRS = np.array([metric_res[m][wght]['BINS_ID'][indj]['SIGMA'] for indj in indjs]) #err from SLR ReSample\n",
    "        errCV = np.array([sample_var_truez[m][wght]['BINS_ID'][indj]['SIGMA'] for indj in indjs]) #err from CosVar to photo-z\n",
    "        \n",
    "        \n",
    "        err = np.sqrt(errRS**2 + errCV**2 ) #add in quadrature\n",
    "        y = np.array([metric_res[m][wght]['BINS_ID'][indj]['VALUE'] for indj in indjs])\n",
    "        \n",
    "        #add offsets to x-axis plotting for viewability\n",
    "        plt.errorbar(np.array(z)-0.01, y, yerr=errRS, fmt='^', markersize=str(Ms), elinewidth=lw, label='SLR err')\n",
    "        plt.errorbar(np.array(z), y, yerr=errCV , fmt='*', markersize=str(Ms), elinewidth=lw, label='Sample Var err')\n",
    "        plt.errorbar(np.array(z)+0.01, y, yerr=err, fmt='o', markersize=str(Ms), elinewidth=lw, label='Combined errs')\n",
    "\n",
    "        #get requirements if they exist:\n",
    "        if m in requirements:\n",
    "\n",
    "            err_req = requirements[m]['error'].scaling(np.array(z))\n",
    "            plt.errorbar(np.array(z)+0.02, y, yerr=err_req, fmt=',',markersize=str(Ms), elinewidth=lw, label='Error req.')\n",
    "\n",
    "            val_req = requirements[m]['value'].scaling(np.array(z))\n",
    "            plt.plot(np.array(np.sort(z)), val_req,'--', linewidth=3,label='Value req.')\n",
    "\n",
    "        plt.xlabel('Tomographic redshift bin')\n",
    "        plt.title('{:} weight={:}'.format(label.replace('.', ' '), wght))\n",
    "        plt.ylim(plt_range[m])\n",
    "        plt.ylabel(metric_latex[m])\n",
    "        plt.legend(loc=2) \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(label + '.' + wght + '.' + m + '.pdf')\n",
    "        print (\" \")\n",
    "        #print ('weight: {:}'.format(wght))\n",
    "        print ('metric: | {:} | {:}'.format(m, metric_description[m]))\n",
    "        mean = metric_res[m][wght]['VALUES']\n",
    "        rsErr = metric_res[m][wght]['SIGMA']\n",
    "        cosVarErr = sample_var_truez[m][wght]['SIGMA']\n",
    "        err_ = np.sqrt(rsErr**2 + cosVarErr**2)\n",
    "        if show_z:\n",
    "            print ('Mean & Combined Err &  SLR Err & CosVar Err & \\\\\\\\')\n",
    "        print ('{:0.4f} \\pm {:0.4f}  ( {:0.1e} \\pm {:0.1e}) \\\\\\\\ '.format(mean, err_, rsErr, cosVarErr))\n",
    "        print (\" \")\n",
    "        if show_z:\n",
    "            print ('Per tomographic bins')\n",
    "            print ('<z>: ', ','.join(['{:0.4}'.format(i) for i in z]))\n",
    "            show_z = False\n",
    "            \n",
    "        print (m + ': ', ','.join(['{:0.4}'.format(i) for i in y]))\n",
    "        print (m + '_err: ', ','.join(['{:0.4}'.format(i) for i in err]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h4>WL only metric delta_sigma_crit</h4>\n",
    "\n",
    "One weak lensing metric behaves differnetly from all the others and must be analysed separately here. These cells won't work unless we are looking at the WL science sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if science_sample == 'WL':\n",
    "    \n",
    "    #get cos variance terms\n",
    "    cos_var = [cosvar[i][bns_str]['Z']['Z_MC']['vlfn.delta_sigma_crit']['IN_WL_SAMPLE']['value'] for i in range(len(cosvar))]\n",
    "    \n",
    "    #get paths to metrics\n",
    "    metric_ = {'delta_sigma_crit': ['Z_MC', 'vlfn.delta_sigma_crit'] }\n",
    "    \n",
    "    z_lens = res[files[0]][metric_['delta_sigma_crit'][0]][metric_['delta_sigma_crit'][1]]['WEIGHT_ZLENS_0.7']['value']\n",
    "    z_lens = np.sort(z_lens.keys())\n",
    "    \n",
    "    cos_var_err = {}\n",
    "    for i in z_lens:\n",
    "        cos_var_err['{:0.2}'.format(i)] = np.std([j[i] for j in cos_var])\n",
    "\n",
    "    #print cos_var_err\n",
    "\n",
    "    files = [i for i in res.keys() if i !='test_config']\n",
    "\n",
    "\n",
    "    print ('delta_sigma_crit metric: see bh_photo_z_validation.py delta_sigma_crit() ')\n",
    "    \n",
    "\n",
    "    values = np.zeros((len(files), len(z_lens)))\n",
    "    for i, fil in enumerate(files):\n",
    "        values[i] = [res[fil]['Z_MC']['vlfn.delta_sigma_crit']['WEIGHT_ZLENS_{:0.2}'.format(zi)]['value'][zi] for zi in z_lens]\n",
    "    mn, std = np.mean(values, axis=0), np.std(values, axis=0)\n",
    "    for i in range(len(z_lens)):\n",
    "        std_cosvar = np.sqrt(std[i]**2 + cos_var_err['{:0.2}'.format(z_lens[i])]**2 )\n",
    "        print ('z_lens: {:}  delta_sigma_crit {:0.4} \\pm {:0.4} [RsErr {:0.4}]'.format(z_lens[i], mn[i], std_cosvar, std[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>You're done!</h3>\n",
    "\n",
    "Please share the results with the redshift wg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generating the sample (cosmic) variance</h3>\n",
    "\n",
    "This code snippet loads in a heap of validation script outputs, measured on each patch in a simulation, and stores the resulting list in the cosmic variance file in the validation directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = glob.glob('/Users/hoyleb/DATA/DES/PHOTOZ/COSMIC_VAR/cos_0alh*.p')\n",
    "files = glob.glob('/Users/hoyleb/Sites/DES/PHOTOZ/MASKS/rotated_masks/cos_0alh*.p')\n",
    "files = [i for i in files if 'Alh' not in i and 'e5' not in i]\n",
    "res = []\n",
    "for fil in files:\n",
    "    r = pickle.load(open(fil, 'r'))\n",
    "    res.append(r)\n",
    "print len(res)\n",
    "pickle.dump(res, bz2.BZ2File(path + '/cosmic_variance_data/cosvariance_metrics.cos0alh.p.bz2', 'w'))\n",
    "\n",
    "1 / 0\n",
    "files = glob.glob('/Users/hoyleb/DATA/DES/PHOTOZ/COSMIC_VAR/cos_1alh*.p')\n",
    "res = []\n",
    "for fil in files:\n",
    "    r = pickle.load(open(fil, 'r'))\n",
    "    res.append(r)\n",
    "print len(res)\n",
    "pickle.dump(res, bz2.BZ2File(path + '/cosmic_variance_data/cosvariance_metrics.cos1alh.p.bz2', 'w'))\n",
    "\n",
    "files = glob.glob('/Users/hoyleb/DATA/DES/PHOTOZ/COSMIC_VAR/cos_2alh*.p')\n",
    "files = [i for i in files if 'e5' not in i]\n",
    "res = []\n",
    "for fil in files:\n",
    "    r = pickle.load(open(fil, 'r'))\n",
    "    res.append(r)\n",
    "print len(res)\n",
    "pickle.dump(res, bz2.BZ2File(path + '/cosmic_variance_data/cosvariance_metrics.cos2alh.p.bz2', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "cos_noalh = copy.deepcopy(cos_vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "mm = ['wl_metric', 'sigma_68', 'median', 'outFrac_2sigma68']\n",
    "\n",
    "for m in mm:\n",
    "    plt.plot(z, (cos_vv[m] - cos_noalh[m])*100/cos_vv[m], label=m)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Tomographic bin')\n",
    "plt.ylabel('CosVar uncertainty Reduction with Y3Alh')\n",
    "f = plt.figure()\n",
    "cnt=0\n",
    "col = ['red', 'blue', 'green', 'yellow']\n",
    "for m in mm:\n",
    "    plt.plot(z, cos_noalh[m] , '--',color=col[cnt], label='cos' + m)\n",
    "    plt.plot(z, cos_vv[m] ,color=col[cnt],  label='cos2alh' + m, linewidth=2 )\n",
    "    \n",
    "        #get requirements:\n",
    "    if m in requirements:\n",
    "        err_req = requirements[m]['error'].scaling(np.array(z))\n",
    "        plt.plot(np.array(z), err_req,'o-', color=col[cnt], label='Error req.', linewidth=1 )\n",
    "    cnt += 1    \n",
    "plt.legend()\n",
    "plt.xlabel('Tomographic bin')\n",
    "plt.ylabel('CosVar uncertainty [Cos field]')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Validation on 1x cosmos-like simulation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = pickle.load(bz2.BZ2File(path + '/cosmic_variance_data/cosvariance_metrics.cos0alh.p.bz2', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = ['WL_bins1', 'WL_bins2', 'LSS']\n",
    "ssample = ['IN_WL_SAMPLE', 'IN_WL_SAMPLE', 'IN_LSS_SAMPLE']\n",
    "for ii, i in enumerate(res[0]):\n",
    "    print i\n",
    "    r_ = res[0][i]['Z']['Z_MC']['vlfn.wl_metric'][ssample[ii]]['bins']['MEAN_Z']\n",
    "    error = np.array([j[i]['Z']['Z_MC']['vlfn.wl_metric'][ssample[ii]]['bins']['MEAN_Z']['value'] for j in res])\n",
    "    err = np.std(error, axis=0)\n",
    "    plt.errorbar(r_['bin_center'], r_['value'], yerr=err, fmt='o', alpha=0.9, label=labels[ii])\n",
    "    if ii ==1:\n",
    "        print ', '.join(['{:0.3}'.format(k) for k in r_['bin_center']])\n",
    "        print ', '.join(['{:0.3}'.format(k) for k in r_['value']])\n",
    "        print ', '.join(['{:0.3}'.format(k) for k in err])\n",
    "plt.legend()\n",
    "plt.xlabel('Redshift')\n",
    "plt.ylabel('|<z_true>-<z_mc>|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Bin, Zmc , truth\n",
    "1, 0.35, 0.29\n",
    "2, 0.52, 0.44\n",
    "3, 0.75, 0.70\n",
    "4, 1.01, 0.94\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'bin1 true delta_z', 0.35 - 0.29,'valid:', 0.0761, '+-',0.0105\n",
    "print 'bin2 true delta_z',0.52 - 0.44, 'valid:',0.102, '+-',0.013\n",
    "print 'bin3 true delta_z',0.75- 0.70,  'valid:',0.0531, '+-', 0.0135\n",
    "print 'bin4 true delta_z',1.01 - 0.94, 'valid:',0.0509,'+-', 0.0212"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(0.08 - 0.102)/0.013, (0.06 -0.07610)/ 0.0105, 'sigma away from the truth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
