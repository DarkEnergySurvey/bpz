test_name: photoz-wg
#these are the point prediction tests
point:
    #what is the true redshift that we will compare with?
    truths: REDSHIFT

    #shall we use weights when calculating metrics, if so specify here.
    #:  no weights, or list of weights
    weights: [IN_SCIENCE_SAMPLE, WL_WEIGHT]

    #what metrics do we want to measure. "numpy.std" is the standard deviation from numpy
    # and "bh_photo_z_validation.sigma_68" is the sigma_68 metric found in the bh_photo_z_validation.py file
    metrics_diffz: {'MEAN_Z': [numpy.median, bh_photo_z_validation.sigma_68, bh_photo_z_validation.outlier_fraction, bh_photo_z_validation.outFrac_2sigma68, bh_photo_z_validation.outFrac_3sigma68]}

    metrics_z1_z2: {'Z_MC' : [bh_photo_z_validation.wl_metric, bh_photo_z_validation.delta_sigma_crit]}

    #for WL we also care about measuring some quantities as a function of lens Z
    z_lens: [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]

    #Finally do we want to also measure the metrics in some "bins". 
    #we define the column_name: 'string of bins / string of function that makes bins'
    bins: [MEAN_Z: '[0.0, 0.1, 0.2, 0.43, 0.63, 0.9, 1.3]']
    #'[0, 0.1, 0.2, 0.43, 0.63, 0.9, 1.3]']
    #'[0, 0.1, 0.2, 0.39, 0.45, 0.58, 0.75, 1.3]']
    #Should we calculate errors on each metric? if yes state how
    #you can include as many different error functions as you like.
    #e.g. pval.bootstrap_mean_error_binned == boostrap resampled errors
    error_function:

#these are the pdf tests
pdf: 
    #we can examine individual redshift pdfs. Remove this part you don't need to compare

    #or shall we compare against stacked pdfs
    stacks:
        truths: REDSHIFT

        #which additional bins shall we use to calculate metrics?
        bins: [REDSHIFT: '[0.0, 0.3, 0.6, 0.9, 1.3, 2.0]']
        metrics: [bh_photo_z_validation.ks_test, bh_photo_z_validation.npoisson, bh_photo_z_validation.log_loss, numpy.mean]

        #shall we use weights when calculating metrics, if so specify here.
        weights:

