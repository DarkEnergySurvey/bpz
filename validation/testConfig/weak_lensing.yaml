test_name: wl-wg
#these are the point prediction tests
point:
    #which photo-z predictions do we want to test
    predictions: [MEAN_Z, Z_MC]
    
    #what is the true redshift that we will compare with?
    truths: Z_SPEC

    weights: WEIGHTS

    #what metrics do we want to measure. "numpy.std" is the standard deviation from numpy
    # and "bh_photo_z_validation.sigma_68" is the sigma_68 metric found in the bh_photo_z_validation.py file
    metrics: [numpy.median, bh_photo_z_validation.sigma_68, bh_photo_z_validation.outlier_fraction]
    
    #do we want to assign an accetable tolerance to each of these tests?
    tolerance: [0.4, 0.001, 0.03, 10]
    
    #Finally do we want to also measure the metrics in some "bins". 
    #we define the column_name: 'string of bins / string of function that makes bins'
    bins: [MEAN_Z: '[0.0, 0.3, 0.6, 0.9, 1.3, 2.0]']

    #Should we calculate errors on each metric? if yes state how
    #you can include as many different error functions as you like.
    error_function: [bh_photo_z_validation.bootstrap_mean_error]
    
#these are the pdf tests
pdf:
    #we can examine individual redshift pdfs
    individual:
        truths: Z_SPEC
        bins: [MAG_DETMODEL_I: '[ 17.5, 19, 22, 25]']
        metrics: [bh_photo_z_validation.Bordoloi_pdf_test]
        tolerance: [0.7, 20]
        #shall we use weights when calculating metrics, if so specify here.
        weights: WEIGHT
    #or shall we compare against stacked pdfs
    stacks:
        truths: Z_SPEC
        truth_bins: [Z_SPEC: 'numpy.arange(5)*0.33']
        bins: [MAG_DETMODEL_I: '[ 17.5, 19, 22, 25]']
        metrics: [bh_photo_z_validation.ks_test, bh_photo_z_validation.log_loss]
        tolerance: [0.7, 20]
        #shall we use weights when calculating metrics, if so specify here.
        weights: WEIGHT


