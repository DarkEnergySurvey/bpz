test_name: photoz-wg
#these are the point prediction tests
point:
    #which photo-z predictions do we want to test
    predictions: [MEAN_Z, Z_MC, MODE_Z]
    #MEDIAN_Z, MODE_Z, MEAN_Z]
    
    #what is the true redshift that we will compare with?
    truths: REDSHIFT
    
    #shall we use weights when calculating metrics, if so specify here.
    weights: 

    #what metrics do we want to measure. "numpy.std" is the standard deviation from numpy
    # and "bh_photo_z_validation.sigma_68" is the sigma_68 metric found in the bh_photo_z_validation.py file
    metrics_diffz: [numpy.median, bh_photo_z_validation.sigma_68, bh_photo_z_validation.outlier_fraction, bh_photo_z_validation.outFrac_2sigma68, bh_photo_z_validation.outFrac_3sigma68]

    metrics_z1_z2: [bh_photo_z_validation.wl_metric]

    #do we want to assign an accetable tolerance to each of these tests?
    tolerance:
    
    #Finally do we want to also measure the metrics in some "bins". 
    #we define the column_name: 'string of bins / string of function that makes bins'
    bins: [MEAN_Z: '[0.0, 0.1, 0.2, 0.43, 0.63, 0.9, 1.3]']
    #'[0, 0.1, 0.2, 0.43, 0.63, 0.9, 1.3]']
    #'[0, 0.1, 0.2, 0.39, 0.45, 0.58, 0.75, 1.3]']
    #Should we calculate errors on each metric? if yes state how
    #you can include as many different error functions as you like.
    error_function:
    
#these are the pdf tests
pdf: 
    #we can examine individual redshift pdfs. Remove this part you don't need to compare

    #or shall we compare against stacked pdfs
    stacks:
        truths: REDSHIFT

        #which additional bins shall we use to calculate metrics?
        bins: [REDSHIFT: '[0.0, 0.3, 0.6, 0.9, 1.3, 2.0]']
        metrics: [bh_photo_z_validation.ks_test, bh_photo_z_validation.npoisson, bh_photo_z_validation.log_loss, numpy.mean]

        #shall we use weights when calculating metrics, if so specify here.
        weights:

