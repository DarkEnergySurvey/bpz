{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Example validation script</h1>\n",
    "<p>In this notebook we walk you through the steps of validating your point predictions and pdfs. We will use the same codes that will be used in the final validation stage. To make this script realistic, with realistic run-timess, we have around 10,000 galaxies in our testing files.</p>\n",
    "<p>First, ensure that your repo is up to date, by running this command</p>\n",
    "<code>cd /path/to/photoz-wg/</code>\n",
    "\n",
    "Followed by \n",
    "\n",
    "<code>git pull</code>\n",
    "\n",
    "<h3>Unit tests</h3>\n",
    "<p>You are welcome to enter into the validation/tests/ directory and run the unit tests using, e.g.,\n",
    "\n",
    "<code>%>nosetests </code>\n",
    "\n",
    "<p>If you would like to expand the unit tests, please do so! and push your changes to a branch. The more tests we have, the more we can belive our codes!</p>\n",
    "\n",
    "\n",
    "<h3>The validation script</h3>\n",
    "<p>The validation script is found in the validation directory. You may ignore this notebook, and call the validation script directly from the directory (or anywhere if you add the directory to your path variable). This will instantly check that your file is well formed, and then perform all the standard tests, and output the scores.</p>\n",
    "\n",
    "usage like\n",
    "\n",
    "\n",
    "<code>%>photoz_metrics.py data/PointPredictions1.fits</code>\n",
    "\n",
    "\n",
    "or to do many pdfs predictions at a time\n",
    "\n",
    "<code>%>photoz_metrics.py data/pdfPredictions*.hdf5</code>\n",
    "\n",
    "\n",
    "or a mix of the two, many point prediction files, and many pdf files\n",
    "\n",
    "<code>%>photoz_metrics.py data/pdfPredictions\\*.hdf5 data/PointPredictions\\*.fits</code>\n",
    "\n",
    "\n",
    "\n",
    "or you can make more fine tuned validations using a configuration YaML file\n",
    "\n",
    "\n",
    "<code>%>photoz_metrics.py yourValidationConfig.yaml </code>\n",
    "\n",
    "\n",
    "\n",
    "<h4>help file</h4>\n",
    "If your just call it like this:\n",
    "\n",
    "<code>%>photoz_metrics.py</code> \n",
    "\n",
    "\n",
    "\n",
    "We will write a example YaML file \"exampleValidation.yaml\"  to the working directory.\n",
    "\n",
    "<h3>The validation code</h3>\n",
    "<p>Dependecies, pandas, astropy, pyYaml</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some dependencies. We'll need YAML (pip install pyYaml)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "#doubled up for use later!\n",
    "import numpy as numpy\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "#let's do some speed-testing\n",
    "import time\n",
    "\n",
    "#what is the path to DES photo-z wg bucket/validation?\n",
    "#for Ben this is:\n",
    "#modify this for your system\n",
    "path_bh = '../validation/'\n",
    "\n",
    "sys.path.append(path_bh)\n",
    "\n",
    "import bh_photo_z_validation as pval\n",
    "\n",
    "from weighted_kde import gaussian_kde as gss_kde\n",
    "\n",
    "#finally some plotting routines.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load the point predictions files</h3>\n",
    "<p>We assume that you already have the data in the correct format. This means it has the required point prediction redshift estimates, and Z_SPEC, and COADD_OBJECTS_ID and MAG_DETMODEL_I. We will now perform some \"unit-tests\" to ensure this.</p>\n",
    "\n",
    "<p>In fact, in the code, we identify all the columnns that will be used in the specified tests, and check for their existence too!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#change this to your fits file path\n",
    "pointPredictionFitsPath = path_bh + 'tests/data/validPointPrediction.fits'\n",
    "\n",
    "require_cols = ['Z_SPEC', 'COADD_OBJECTS_ID', 'MAG_DETMODEL_I', 'Z_MC']\n",
    "\n",
    "#lets start the speed test counter!\n",
    "t1 = time.time()\n",
    "\n",
    "#let's check the file is valid, has the required columns, and load it in.\n",
    "okay, dataFrame = pval.valid_file(pointPredictionFitsPath, require_cols)\n",
    "\n",
    "#if it's not okay, we tell you why in the error message (now stored in dataFrame)\n",
    "if okay is False:\n",
    "    print \"the file is not in the correct format\"\n",
    "    print \"erorr message: \" + dataFrame\n",
    "\n",
    "print \"data loaded in %0.4f secs for %0.1d galaxies\" % (time.time() - t1, len(dataFrame))\n",
    "print dataFrame[0:2]\n",
    "print \"number of objects in our test sample\", len(dataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define the tests</h3>\n",
    "<p>We next will build a test. We write each test as a really lovely YaML file. YaML is desgined to be easy for humans to read. You'll understand how easy each test becomes, in a few seconds. </p>\n",
    "\n",
    "<p>Each test determines which metrics (such as $\\sigma_{68}$ or the K-S test) will be measured, and optionally if we want to measure the metric by binning the data along one of the columns, e.g. if we want to bin along MAG_DETMODEL_I. We can also decide to set an allowed \"tolerance\", this informs us if the tests passed with the required precision</p>\n",
    "\n",
    "<p>You don't need to write any tests if you don't want to. The standard ones are included already when you run the script from the command line. Below I shown an exmaple of what a test looks like. You can include as many tests, from as many sources as you like. Each test it it's own yaml file.</p>\n",
    "\n",
    "<p>The base tests can be found here </p>\n",
    "\n",
    "<code>%>ls photoz-wg/validation/testConfig/*.yaml</code>\n",
    "\n",
    "And looks like this: [The comments '#' are also included in the file, for easy reading!]\n",
    "\n",
    "===== begin YaML file =======\n",
    "\n",
    "#First let us give this test a name, to differeniat it from other tests\n",
    "\n",
    "test_name: example_test1\n",
    "\n",
    "#paths to file locations. will assume '.fits' as point predictions '.hdf5' as pdf predictions, add more files to list to compare multiple files\n",
    "\n",
    "\n",
    "filePaths: ['tests/data/validPointPrediction.fits', 'tests/data/validHDF.hdf5']\n",
    "\n",
    "#Which metrics and tolerance should we measure either a list of metrics, such as and or a precomputed collection of group metrics and tolerances set blank, or delete this line to not use these preconfigured metrics/bins/tolerances\n",
    "\n",
    "standardPredictions: [/testConfig/photoz.yaml, /testConfig/weak_lensing.yaml]\n",
    "\n",
    "#what will the path/ and or/base file name of the results be called?\n",
    "\n",
    "resultsFilePrefix: myResultsOutput\n",
    "\n",
    "#And or / additionally choose your own metrics, as list\n",
    "#remove these if not required\n",
    "#these are the point prediction tests\n",
    "\n",
    "point:\n",
    "    \n",
    "    #which photo-z predictions do we want to test\n",
    "    predictions: [MODE_Z, MEAN_Z, Z_MC]\n",
    "    \n",
    "    #what is the true redshift that we will compare with?\n",
    "    truths: Z_SPEC\n",
    "    \n",
    "    #should we calculated weighted metrics where available?\n",
    "    weights: WEIGHTS\n",
    "\n",
    "    #what metrics do we want to measure. \"numpy.std\" is the standard deviation from numpy\n",
    "    \n",
    "    # and \"bh_photo_z_validation.sigma_68\" is the sigma_68 metric found in the bh_photo_z_validation.py file\n",
    "    \n",
    "    metrics: [numpy.std, numpy.median, bh_photo_z_validation.sigma_68, bh_photo_z_validation.outlier_fraction]\n",
    "    \n",
    "    #do we want to assign an accetable tolerance to each of these tests?\n",
    "    tolerance: [0.4, 0.001, 0.02, 5]\n",
    "    \n",
    "    #Finally do we want to also measure the metrics in some \"bins\".\n",
    "    #we define the column_name: 'string of bins / string of function that makes bins'\n",
    "    bins: [MAG_DETMODEL_I: '[10, 15, 20, 25, 30]', MODE_Z: 'numpy.linspace(0, 2, 20)']\n",
    "\n",
    "    #Should we calculate errors on each metric? if yes state how\n",
    "    #you can include as many different error functions as you like.\n",
    "    error_function: [bh_photo_z_validation.bootstrap_mean_error]\n",
    "     \n",
    "#these are the pdf tests\n",
    "pdf: \n",
    "    #we can examine individual redshift pdfs. Remove this part you don't need to compare\n",
    "    individual:\n",
    "        truths: Z_SPEC\n",
    "      \n",
    "        #one statistic is calcualted in bh_photo_z_validation.py eval_pdf_point(), add your own at will.\n",
    "        metrics: [bh_photo_z_validation.eval_pdf_point]\n",
    "        bins: [MAG_DETMODEL_I: '[ 17.5, 19, 22, 25]']\n",
    "        tolerance: [0.7, 20]\n",
    "        #shall we use weights when calculating metrics, if so specify here.\n",
    "        weights: WEIGHTS\n",
    "        \n",
    "    #or shall we compare against stacked pdfs\n",
    "    stacks:\n",
    "        truths: Z_SPEC\n",
    "\n",
    "        #which additional bins shall we use to calculate metrics?\n",
    "        bins:\n",
    "        metrics: [bh_photo_z_validation.ks_test, bh_photo_z_validation.npoisson, bh_photo_z_validation.log_loss]\n",
    "\n",
    "        #shall we use weights when calculating metrics, if so specify here.\n",
    "        weights: weights_valid\n",
    "\n",
    "\n",
    "\n",
    "===== end YaML file =======\n",
    "\n",
    "You see that we have now defined a set of tests for both point predictions, and pdfs.\n",
    "\n",
    "\n",
    "<h3>Loading the tests</h3>\n",
    "<p>We load the tests like this!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testYamlPath = path_bh + 'testConfig/photoz.yaml'\n",
    "testConfig = yaml.load(open(testYamlPath, 'r'))\n",
    "\n",
    "#check that the test is valid. This is die if it's not.\n",
    "isTestValid = pval.valid_tests(testConfig)\n",
    "\n",
    "#the YaML file is parsed nicely to a python dictionary!\n",
    "print \"This test is called: \" + testConfig['test_name'] + '\\n'\n",
    "print testConfig\n",
    "print \"\\n\\nExample of extracting a statistic to measure\\n\"\n",
    "print testConfig['point']['metrics'][1]\n",
    "print \"\\nExample of which error we can assign to this metric\\n\"\n",
    "print testConfig['point']['error_function'][0]\n",
    "\n",
    "print \"\\nThis means look in bh_photo_z_validation.py to see this function. You can add your own error function too!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>YaML corrupt?</h3>\n",
    "<p>If your YaML file doesn't work, copy and check it here: http://yaml-online-parser.appspot.com\n",
    "\n",
    "<h3>Running a test for point predictions</h3>\n",
    "<p>Now we can play some testing magic. One thing to note, is they Python is <b>friggin</b> awesome. Remember we wrote the metric like this: 'bh_photo_z_validation.sigma_68', well we can turn this into an executable using a function found in bh_photo_z_validation.py called get_function(). We'll see this in the below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pointTestConfig = testConfig['point']\n",
    "for eachPointPrediction in pointTestConfig['predictions']:\n",
    "    #let's calcalate the redshift scaled residuals Deltaz = (z - photz) / (1 + z)\n",
    "    deltaz_1pz = pval.delta_z_1pz(dataFrame[pointTestConfig['truths']], dataFrame[eachPointPrediction])\n",
    "    \n",
    "    #are we adding weights to each galaxy prediction?\n",
    "    weights = dataFrame[pointTestConfig['weights']]\n",
    "    \n",
    "    #now let's calculate the value of each chosen metric\n",
    "    for i, eachMetric in enumerate(pointTestConfig['metrics']):\n",
    "        \n",
    "        #amazing metric string to function conversion. \n",
    "        #Add your own functions by putting them in bh_photo_z_validation.py\n",
    "        metric_function = pval.get_function(eachMetric)\n",
    "        metric_value =  metric_function(deltaz_1pz)\n",
    "        \n",
    "        t1 = time.time()\n",
    "        #no add our generic error function, we'll need weights for this\n",
    "        error_function = pval.get_function(pointTestConfig['error_function'][0])\n",
    "        print \"\\n200 BootStrap errors calculated in  in %0.6f secs for %0.1d galaxies\" % (time.time() - t1, len(dataFrame))\n",
    "        \n",
    "        error_value = error_function(deltaz_1pz, weights, metric_function)\n",
    "        \n",
    "        print \" \\n\"\n",
    "        print \"using file: \" + pointPredictionFitsPath \n",
    "        print \"point prediction: \" + eachPointPrediction \n",
    "        print \"we measure the statistic: \" + eachMetric.split('.')[-1] \n",
    "        print \"and get the value: \" + str(metric_value) + \" [or error and mean from bootstrap]\", error_value\n",
    "\n",
    "# All the results of these tests will be printed to the screen below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>pdf tests</h2>\n",
    "<p>Now let's turn our attention to the pdf tests. Let's load them and look at them.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdfTestConfig = testConfig['pdf']\n",
    "print pdfTestConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Load pdf data</h3>\n",
    "<p>Lets load some data, and check that the data file is valid, and contains the columns we will be using.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdfPredictionFitsPath = path_bh + 'tests/data/validHDF.hdf5'\n",
    "\n",
    "require_cols = ['Z_SPEC', 'COADD_OBJECTS_ID', 'MAG_DETMODEL_I']\n",
    "\n",
    "t1 = time.time()\n",
    "okay, dataFrame = pval.valid_file(pdfPredictionFitsPath, require_cols)\n",
    "print \"\\npdf file loaded in %0.4f secs for %0.1d galaxies\" % (time.time() - t1, len(dataFrame))\n",
    "  \n",
    "if okay is False:\n",
    "    print \"the file is not in the correct format\"\n",
    "    print \"erorr message: \" + dataFrame\n",
    "#print dataFrame[0:1], 'lenght', len(dataFrame)\n",
    "print dataFrame.info()\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>pdf array</h3>\n",
    "Let's extract the pdfs as an N-darray of shape (ngals, nbins).\n",
    "\n",
    "We also have access to the bin edges using the pdf_key valus<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zcols = [c for c in dataFrame.keys() if 'pdf_' in c]\n",
    "zbins = np.array([float(c.split('f_')[-1]) for c in zcols])\n",
    "pdf_z_center = zbins + (zbins[1] - zbins[0]) / 2.0\n",
    "print \"pdf bin centers\", pdf_z_center[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's get the pdfs as an nunmpy array\n",
    "pdfs = dataFrame[zcols].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Weighted pdfs</h3>\n",
    "Let's stack all pdfs using the input weights (note, this is all random data, so don't expect much!). We'll use tools found in bh_photo_z_validation.py to do all the heavy lifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "t1 = time.time()          \n",
    "#stack all pdfs across all gals and use the weights\n",
    "stackedPdf = pval.stackpdfs(pdfs, weights=weights)\n",
    "\n",
    "#normalise this stack\n",
    "normStackedPdf = pval.normalisepdfs(stackedPdf, pdf_z_center)\n",
    "\n",
    "print \"\\n All computations took %0.4f secs for %0.1d galaxies\" % (time.time() - t1, len(pdfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tests using points and pdf estimates (on our fake data)</h3>\n",
    "<p> Do note we are just using some meaningless, random, made up data.</p>\n",
    "<p>Let's continue with the tutorial and perform some other comparisons on these (meaningless) distributions!</p>\n",
    "<p>First we can ask how much of each galaxy's pdf sits within some bin (0.1 < z < 0.3)<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "pdf_in_int = pval.integrate_dist_bin(pdfs,pdf_z_center, 0.1, 0.3)\n",
    "print \"\\n All computations took %0.4f secs for %0.1d galaxies\" % (time.time() - t1, len(pdfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now, let's plot a few of those with lots of pdf in the bin</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = pdf_in_int > 0.95\n",
    "for j in range(4):\n",
    "    plt.plot(pdf_z_center, pdfs[i][j])\n",
    "plt.plot([0.1,0.1], [0,60], '--b')\n",
    "plt.plot([0.3,0.3], [0,60], '--b')\n",
    "plt.xlabel('z')\n",
    "plt.xlim(0,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and plot a few with only a little weight in the bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = (pdf_in_int > 0.3) * (pdf_in_int < 0.31)\n",
    "for j in range(4):\n",
    "    plt.plot(pdf_z_center, pdfs[i][j])\n",
    "plt.plot([0.1,0.1], [0,10], '--b')\n",
    "plt.plot([0.3,0.3], [0,10], '--b')\n",
    "plt.xlabel('z')\n",
    "plt.xlim(0.08,0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Bordoloi pdf test</h3>\n",
    "<p>One test that we perform is the Bordoloi et al 2012 http://arxiv.org/pdf/1201.0995.pdf test. The idea is to make a cumulative pdf for each galaxy, and then calculate the y-axis value that corresponds to the x-axis value of the spectroscopic (or other) redshift.\n",
    "<br>\n",
    "This statistics, averaged over many galaxies, should have a flat distribution in y-axis values (between 0-1). To check how flat it is, we can us the gini criteria! [https://en.wikipedia.org/wiki/Gini_coefficient]\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "Z_SPEC = dataFrame['Z_SPEC']\n",
    "\n",
    "nomalisepdfs = pval.normalisepdfs(pdfs, zbins)\n",
    "y_axis_vals = pval.cumaltive_to_point(nomalisepdfs, zbins, Z_SPEC)\n",
    "\n",
    "#what do these look like?\n",
    "#now let's make a histogram of this distribution\n",
    "N=100\n",
    "_ = plt.hist(y_axis_vals,bins=N)\n",
    "plt.ylim(0,100)\n",
    "\n",
    "\n",
    "#finally, let's measure how far away this is from a flat distribution; with gini = 0.0\n",
    "gini_val = pval.gini(np.histogram(y_axis_vals, bins=N)[0] * 1.0)\n",
    "print gini_val\n",
    "print \"this is not really flat (for this made up data)! let's check it out\"\n",
    "cum = np.cumsum(np.histogram(y_axis_vals, bins=N)[0])\n",
    "cum = cum / float(cum[-1])\n",
    "f = plt.figure()\n",
    "plt.plot(np.arange(len(cum)), cum, label= 'this dist. gini= %0.3f' % gini_val)\n",
    "plt.plot(np.arange(len(cum)), np.arange(len(cum))*1.0/len(cum), label='perfect gini=0')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('z bins')\n",
    "plt.ylabel('c-pdf value')\n",
    "print \"\\n All computations (including plots) took %0.4f secs for %0.1d galaxies\" % (time.time() - t1, len(pdfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Boot strap error on these stats</h4>\n",
    "<p>We can again calculate a boot strap error, using a wrapper written in bh_photo_z_validation.py called <code>bootstrap_mean_error_pdf_point()</code>. You are welcome to write your own, and call is as before (yourFileName.yourFunctionName). Let's first get hold of the weights, and then run the internal functions. These calculations are a little slow ~30secs on my computer (for 10k galaxies) </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "weights = np.array(dataFrame['weights_valid'])\n",
    "meanErr = pval.bootstrap_mean_error_pdf_point(nomalisepdfs, zbins, Z_SPEC, weights, pval.Bordoloi_pdf_test)\n",
    "print \"mean and error on \", len(dataFrame), \" pdfs with 200 BootStrap resamples\", meanErr\n",
    "print \"\\n All computations took %0.4f secs for %0.1d galaxies\" % (time.time() - t1, len(pdfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tests comparing dn/dz vs stacked dn/dz pdf distributions (on our fake data)</h3>\n",
    "<p>We also compare dndz distributions, using either built in metrics, or as before, you can write your own. These metrics take the un-normalised distributions (and should normalise if required). Let's start with our example test yaml file, and extract the tests</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test2pdfs = testConfig['pdf']['stacks']\n",
    "print \"\\nThis means that we will use \" + test2pdfs['truths'] + \" as the truth column, and form dNdz bins using kernal density estimates\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Truth distribution turned into KDE</h4>\n",
    "Let's now use Kernal Density estimation to determine the distribution of truths (whatever that is) redshifts at the pdf bin centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "truths = np.array(dataFrame[test2pdfs['truths']])\n",
    "weights = np.array(dataFrame[test2pdfs['weights']])\n",
    "\n",
    "#properly normalise weights, so that np.random.choice works well.\n",
    "weights = weights / np.sum(weights, dtype=float)\n",
    "print np.shape(truths), np.shape(weights), np.shape(pdf_z_center)\n",
    "kde = gss_kde(truths, weights=weights)\n",
    "truth_dist = kde.evaluate(pdf_z_center)\n",
    "\n",
    "#now let's create a kde distribution using pval.dist_pdf, at the bin center pdf_z_center\n",
    "plt.plot(pdf_z_center, truth_dist)\n",
    "plt.title('KDE distributions')\n",
    "plt.ylabel('density')\n",
    "plt.xlabel('truth redshift')\n",
    "f = plt.figure()\n",
    "plt.plot(pdf_z_center, truth_dist, label='z_pdf (weighted)', linewidth=1, alpha=0.7)\n",
    "plt.plot(pdf_z_center, normStackedPdf, label='normStackedPdf (weighted)', linewidth=2, alpha=0.7)\n",
    "plt.title('Using random test data')\n",
    "plt.xlabel('redshift')\n",
    "plt.ylabel('pdf')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's explore these distributions using some inbuilt tests</h4> \n",
    "<p>Now let's do the same for the pdfs, and perfrom the tests. All of these messy details are hidden from the user by calling the photoz_metric.py script.</p>\n",
    "<p>Now we are ready to perform the comparison test! Just like before, we can *also* make the test by binning in a choice of features. See the yaml file for more details.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "print 'Npoisson value', pval.npoisson(truth_dist, normStackedPdf)\n",
    "print 'kulbachLeiber_bins', pval.kulbachLeiber_bins(truth_dist, normStackedPdf)\n",
    "print 'ks_test', pval.ks_test(truth_dist, normStackedPdf)\n",
    "print 'time taken for all tests %0.4fs' % (time.time() - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Results files</h2>\n",
    "<p>We write the results of the validation script (recall it's called like) </p>\n",
    "\n",
    "<code>%>photoz_metrics.py data/PointPredictions1.fits </code>\n",
    "\n",
    "<p>Using a results File Prefix which you may have set in the testConfiguration Yaml file.\n",
    "\n",
    "<p>Into a lovely YaML file for human easy reading, and also as a pickle file. This means we can pass the outputs into the plotting tool. An example of that is (or will be) in this notebook directory soon! </p>\n",
    "\n",
    "<h3>Output</h3>\n",
    "\n",
    "<p>Example of the output is below:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#in this example we don't have resultsFilePrefix. \n",
    "#print \"test prefix\", testConfig['resultsFilePrefix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#output file name:\n",
    "pointResults = 'point_.p'\n",
    "pointResults = 'point_.yaml'\n",
    "pdfResults = 'pdf_.p'\n",
    "pdfResults = 'pdf_.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The results are formatted to look like this: (in e.g. points_.yaml), and can be read using <code>python pickle</code> or <code>yaml.load()</code> as before </p>\n",
    "\n",
    "<code>\n",
    "tests/data/validPointPrediction.fits:\n",
    "  0:\n",
    "    Z_MC:\n",
    "      bh_photo_z_validation.sigma_68:\n",
    "        delta_z:\n",
    "          VALUE: 0.7035013244853514\n",
    "          bins:\n",
    "            Z_MC:\n",
    "              BIN_CENTERS:\n",
    "              - 0.2593094256649402\n",
    "              - 0.7587431412716394\n",
    "              - 1.4909100996939546\n",
    "              BIN_CENTERS_MEAN_BS:\n",
    "              - 0.2617489006521707\n",
    "              - 0.759508863364966\n",
    "              - 1.4927365449936938\n",
    "              BIN_CENTERS_SIGMA_BS:\n",
    "              - 0.008674035664842377\n",
    "              - 0.009549516216673822\n",
    "              - 0.013034528528585611\n",
    "              MEAN_BS:\n",
    "              - 0.334349762217648\n",
    "              - 0.34009724283466963\n",
    "              - 0.40911843435271694\n",
    "              SIGMA_BS:\n",
    "              - 0.019772862096479635\n",
    "              - 0.02627186735715245\n",
    "              - 0.016312035676113744\n",
    "              VALUE:\n",
    "              - 0.3283407192008526\n",
    "              - 0.32228114188030743\n",
    "              - 0.41986501332174475\n",
    "        diff_1pz:\n",
    "          VALUE: 0.2840504374875773\n",
    "          bins:\n",
    "            Z_MC:\n",
    "              BIN_CENTERS:\n",
    "              - 0.2593094256649402\n",
    "              - 0.7587431412716394\n",
    "              - 1.4909100996939546\n",
    "              BIN_CENTERS_MEAN_BS:\n",
    "              - 0.26151256908410203\n",
    "              - 0.7593840546088471\n",
    "              - 1.492391911786944\n",
    "              BIN_CENTERS_SIGMA_BS:\n",
    "              - 0.00886473342482512\n",
    "              - 0.009578223372819055\n",
    "              - 0.013132219172208937\n",
    "              MEAN_BS:\n",
    "              - 0.08723858288599118\n",
    "              - 0.11446066934486837\n",
    "              - 0.17185082125162893\n",
    "              SIGMA_BS:\n",
    "              - 0.0060519403768464885\n",
    "              - 0.008752563717080358\n",
    "              - 0.006043700892506574\n",
    "              VALUE:\n",
    "              - 0.08452927318731282\n",
    "              - 0.10696974619042877\n",
    "              - 0.17107178338923312\n",
    "\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import fitsio\n",
    "bpz_file = '/Users/Christopher_old/Downloads/y1a1_train_valid_12_11_15_BPZ.fits'\n",
    "data = fitsio.read(bpz_file, ext=2)\n",
    "df_BPZ = pd.DataFrame(data.byteswap().newbyteorder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_binning(df)\n",
    "    bins = [float(name[4:]) for name in df.columns if 'pdf' in name]\n",
    "    dz = (bins[1] - bins[0])/2.0\n",
    "    return bins + dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bh_photo_z_validation as pval\n",
    "import numpy as np\n",
    "folder = '/Users/Christopher_old/ice/data/DES/Y1/validation_hdf5/'\n",
    "ada_file = folder + 'Y1A1_GOLD101_Y1A1trainValid_14.12.2015.validsY1A1.25215.out.DES.pdf.hdf5'\n",
    "dfn_file = folder + 'dnf_validation.hdf5'\n",
    "\n",
    "file_list = [ada_file, dnf_file]\n",
    "code_list = ['ADA_Z_V0.2', 'DNF_V0.1']\n",
    "point_list = ['MODE_Z','MEAN_Z'] # MEDIAN_Z \n",
    "bin_list = [np.linspace(0, 1.4, 4), \n",
    "            np.linspace(0, 1.4, 7)]\n",
    "\n",
    "\n",
    "for file,code in zip(file_list, code_list):\n",
    "    store = pd.HDFStore(file)\n",
    "    df = store['pdf']\n",
    "    if 'dnf' in file:\n",
    "        centers = np.array([float(name[4:]) for name in df.columns if 'pdf' in name])\n",
    "    if '25215' in file:\n",
    "        centers = np.array([float(name[4:]) for name in df.columns if 'pdf' in name]) \n",
    "        centers = centers + centers[1]/2.0\n",
    "    for binning in bin_list:\n",
    "        for selection in point_list:\n",
    "            for_WL0, for_WL1, for_WL2 = pval.weighted_nz_distributions(df, binning=centers, weights='weights_valid', \n",
    "                                                                       tomo_bins=binning, \n",
    "                                                                       z_phot= df[selection].values , n_resample=50)\n",
    "            out_pickle_folder = '/Users/Christopher_old/Dropbox/Y1_validation/Pickle/'\n",
    "            bin_info1 = code + str(len(binning)-1) + '_bins'\n",
    "            bin_info2 = '_' + selection\n",
    "            pickle_file = out_pickle_folder +   bin_info1 + bin_info2 + '.pickle'\n",
    "            pval.ld_writedicts(pickle_file, for_WL2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 28/2))\n",
    "for i in range(len(binning)):\n",
    "    axes.reshape(-1)[i] = plt.plot(centers,for_WL0[i][0])\n",
    "#     axes.flatten()[i] = plt.plot(centers,for_WL0[i][0])\n",
    "#     axes.flatten()[i] = plt.plot(centers,for_WL1[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "axes.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['MEAN_Z'] = data.Z_PHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.to_csv('/Users/Christopher_old/Desktop/RF_S82_point_estimates.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linspace(0,1.4,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linspace(0,1.4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mean = pval.mean(dataFrame, binning=zbins, weights=None, metric='mode', tomo_bins=[0,0.2,0.5, 1.0, 2.0], z_phot= dataFrame.Z_SPEC.values , n_resample=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
