
#test name.
test_name: test1

#paths to file locations. will assume '.fits' as point predictions '.hdf5' as pdf predictions
#add more files to list to compare multiple files
filePaths: ['/home/carnero/Dropbox/DES_photoz_wg/project38/nz_codes/train_1_noweight/annz2_result_des.fits']

#1) OPTIONAL Which metrics and tolerance should we measure either a list of metrics, such as
# and or a precomputed collection of group metrics and tolerances
#set blank, or delete this line to not use these preconfigured metrics/bins/tolerances
standardPredictions:

# what will the path/ and or/base file name of the results be called?
resultsFilePrefix: 'annz2_wl_weights'

#2) EITHER 1) AND OR OPTIONAL Tests here:
#And or / additionally choose your own metrics, as list
#remove these if not required
#these are the point prediction tests
point:
    #which photo-z predictions do we want to test
    predictions: [MEAN_Z]

    #what is the true redshift that we will compare with?
    truths: Z_SPEC

    #should we calculated weighted metrics where available?
    weights: WL_WEIGHTS

    #what metrics do we want to measure. "numpy.std" is the standard deviation from numpy
    # and "bh_photo_z_validation.sigma_68" is the sigma_68 metric found in the bh_photo_z_validation.py file
    metrics: [bh_photo_z_validation.sigma_68]

    #do we want to assign an accetable tolerance to each of these tests?
    tolerance: 

    #Finally do we want to also measure the metrics in some "bins".
    #we define the column_name: 'string of bins / string of function that makes bins'
    bins: [MEAN_Z: 'numpy.linspace(0.4, 1, 7)']

    #Should we calculate errors on each metric? if yes state how
    #you can include as many different error functions as you like. Take care when changing this.
    error_function:

