{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Example validation script</h1>\n",
    "<p>In this notebook we walk you through the steps of validating your point predictions and pdfs. We will use the same codes that will be used in the final validation stage. To make this script realistic, with realistic run-timess, we have around 10,000 galaxies in our testing files.</p>\n",
    "<p>First, ensure that your repo is up to date, by running this command</p>\n",
    "<code>cd /path/to/photoz-wg/</code>\n",
    "\n",
    "Followed by \n",
    "\n",
    "<code>git pull</code>\n",
    "\n",
    "<h3>Unit tests</h3>\n",
    "<p>You are welcome to enter into the validation/tests/ directory and run the unit tests using, e.g.,\n",
    "\n",
    "<code>%>nosetests </code>\n",
    "\n",
    "<p>If you would like to expand the unit tests, please do so! and push your changes to a branch. The more tests we have, the more we can belive our codes!</p>\n",
    "\n",
    "\n",
    "<h3>The validation script</h3>\n",
    "<p>The validation script is found in the validation directory. You may ignore this notebook, and call the validation script directly from the directory (or anywhere if you add the directory to your path variable). This will instantly check that your file is well formed, and then perform all the standard tests, and output the scores.</p>\n",
    "\n",
    "usage like\n",
    "\n",
    "\n",
    "<code>%>photoz_metrics.py data/PointPredictions1.fits</code>\n",
    "\n",
    "\n",
    "or to do many pdfs predictions at a time\n",
    "\n",
    "<code>%>photoz_metrics.py data/pdfPredictions*.hdf5</code>\n",
    "\n",
    "\n",
    "or a mix of the two, many point prediction files, and many pdf files\n",
    "\n",
    "<code>%>photoz_metrics.py data/pdfPredictions\\*.hdf5 data/PointPredictions\\*.fits</code>\n",
    "\n",
    "\n",
    "\n",
    "or you can make more fine tuned validations using a configuration YaML file\n",
    "\n",
    "\n",
    "<code>%>photoz_metrics.py yourValidationConfig.yaml </code>\n",
    "\n",
    "\n",
    "\n",
    "<h4>help file</h4>\n",
    "If your just call it like this:\n",
    "\n",
    "<code>%>photoz_metrics.py</code> \n",
    "\n",
    "\n",
    "\n",
    "We will write a example YaML file \"exampleValidation.yaml\"  to the working directory.\n",
    "\n",
    "<h3>The validation code</h3>\n",
    "<p>Dependecies, pandas, astropy, pyYaml</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some dependencies. We'll need YAML (pip install pyYaml)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "#doubled up for use later!\n",
    "import numpy as numpy\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "#let's do some speed-testing\n",
    "import time\n",
    "\n",
    "#what is the path to DES photo-z wg bucket/validation?\n",
    "#for Ben this is:\n",
    "#modify this for your system\n",
    "path_bh = '../validation/'\n",
    "\n",
    "sys.path.append(path_bh)\n",
    "\n",
    "import bh_photo_z_validation as pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load the point predictions files</h3>\n",
    "<p>We assume that you already have the data in the correct format. This means it has the required point prediction redshift estimates, and Z_SPEC, and COADD_OBJECTS_ID and MAG_DETMODEL_I. We will now perform some \"unit-tests\" to ensure this.</p>\n",
    "\n",
    "<p>In fact, in the code, we identify all the columnns that will be used in the specified tests, and check for their existence too!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded in 0.0496 secs for 10000 galaxies\n",
      "   MEDIAN_Z    COADD_OBJECTS_ID MAG_DETMODEL_I ...     Z_SPEC          Z_MC    \n",
      "-------------- ---------------- -------------- ... -------------- -------------\n",
      "0.753401796191                0  24.3865259087 ...  1.00535707155 1.93576891598\n",
      "0.597140553452                1  20.3067578704 ... 0.991885492609 1.31856938313\n",
      "number of objects in our test sample 10000\n"
     ]
    }
   ],
   "source": [
    "#change this to your fits file path\n",
    "pointPredictionFitsPath = path_bh + 'tests/data/validPointPrediction.fits'\n",
    "\n",
    "require_cols = ['Z_SPEC', 'COADD_OBJECTS_ID', 'MAG_DETMODEL_I', 'Z_MC']\n",
    "\n",
    "#lets start the speed test counter!\n",
    "t1 = time.time()\n",
    "\n",
    "#let's check the file is valid, has the required columns, and load it in.\n",
    "okay, dataFrame = pval.valid_file(pointPredictionFitsPath, require_cols)\n",
    "\n",
    "#if it's not okay, we tell you why in the error message (now stored in dataFrame)\n",
    "if okay is False:\n",
    "    print \"the file is not in the correct format\"\n",
    "    print \"erorr message: \" + dataFrame\n",
    "\n",
    "print \"data loaded in %0.4f secs for %0.1d galaxies\" % (time.time() - t1, len(dataFrame))\n",
    "print dataFrame[0:2]\n",
    "print \"number of objects in our test sample\", len(dataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Define the tests</h3>\n",
    "<p>We next will build a test. We write each test as a really lovely YaML file. YaML is desgined to be easy for humans to read. You'll understand how easy each test becomes, in a few seconds. </p>\n",
    "\n",
    "<p>Each test determines which metrics (such as $\\sigma_{68}$ or the K-S test) will be measured, and optionally if we want to measure the metric by binning the data along one of the columns, e.g. if we want to bin along MAG_DETMODEL_I. We can also decide to set an allowed \"tolerance\", this informs us if the tests passed with the required precision</p>\n",
    "\n",
    "<p>You don't need to write any tests if you don't want to. The standard ones are included already when you run the script from the command line. Below I shown an exmaple of what a test looks like. You can include as many tests, from as many sources as you like. Each test it it's own yaml file.</p>\n",
    "\n",
    "<p>The base tests can be found here </p>\n",
    "\n",
    "<code>%>ls photoz-wg/validation/testConfig/*.yaml</code>\n",
    "\n",
    "And looks like this: [The comments '#' are also included in the file, for easy reading!]\n",
    "\n",
    "===== begin YaML file =======\n",
    "\n",
    "#First let us give this test a name, to differeniat it from other tests\n",
    "\n",
    "test_name: example_test1\n",
    "\n",
    "#paths to file locations. will assume '.fits' as point predictions '.hdf5' as pdf predictions, add more files to list to compare multiple files\n",
    "\n",
    "\n",
    "filePaths: ['tests/data/validPointPrediction.fits', 'tests/data/validHDF.hdf5']\n",
    "\n",
    "#Which metrics and tolerance should we measure either a list of metrics, such as and or a precomputed collection of group metrics and tolerances set blank, or delete this line to not use these preconfigured metrics/bins/tolerances\n",
    "\n",
    "standardPredictions: [/testConfig/photoz.yaml, /testConfig/weak_lensing.yaml]\n",
    "\n",
    "#what will the path/ and or/base file name of the results be called?\n",
    "\n",
    "resultsFilePrefix: myResultsOutput\n",
    "\n",
    "#And or / additionally choose your own metrics, as list\n",
    "#remove these if not required\n",
    "#these are the point prediction tests\n",
    "\n",
    "point:\n",
    "    \n",
    "    #which photo-z predictions do we want to test\n",
    "    predictions: [MODE_Z, MEAN_Z, Z_MC]\n",
    "    \n",
    "    #what is the true redshift that we will compare with?\n",
    "    truths: Z_SPEC\n",
    "    \n",
    "    #should we calculated weighted metrics where available?\n",
    "    weights: WEIGHTS\n",
    "\n",
    "    #what metrics do we want to measure. \"numpy.std\" is the standard deviation from numpy\n",
    "    \n",
    "    # and \"bh_photo_z_validation.sigma_68\" is the sigma_68 metric found in the bh_photo_z_validation.py file\n",
    "    \n",
    "    metrics: [numpy.std, numpy.median, bh_photo_z_validation.sigma_68, bh_photo_z_validation.outlier_fraction]\n",
    "    \n",
    "    #do we want to assign an accetable tolerance to each of these tests?\n",
    "    tolerance: [0.4, 0.001, 0.02, 5]\n",
    "    \n",
    "    #Finally do we want to also measure the metrics in some \"bins\".\n",
    "    #we define the column_name: 'string of bins / string of function that makes bins'\n",
    "    bins: [MAG_DETMODEL_I: '[10, 15, 20, 25, 30]', MODE_Z: 'numpy.linspace(0, 2, 20)']\n",
    "\n",
    "    #Should we calculate errors on each metric? if yes state how\n",
    "    #you can include as many different error functions as you like.\n",
    "    error_function: [bh_photo_z_validation.bootstrap_mean_error]\n",
    "\n",
    "#these are the pdf tests\n",
    "\n",
    "pdf: \n",
    "    #we can examine individual redshift pdfs against a truth value. Remove this part if you don't want this\n",
    "    individual:\n",
    "        truths: Z_SPEC\n",
    "        \n",
    "        #one statistic is calcualted in bh_photo_z_validation.py eval_pdf_point(), add your own at will.\n",
    "        metrics: [bh_photo_z_validation.eval_pdf_point]\n",
    "        bins: [MAG_DETMODEL_I: '[ 17.5, 19, 22, 25]']\n",
    "        tolerance: [0.7, 20]\n",
    "        #shall we use weights when calculating metrics, if so specify here.\n",
    "        weights: WEIGHTS\n",
    "\n",
    "    #or shall we compare against stacked pdfs\n",
    "    stacks:\n",
    "        truths: Z_SPEC\n",
    "        #we convert truths to a distribution by choosing these bins\n",
    "        truth_bins: [Z_SPEC: 'numpy.linspace(0, 2, 4)']\n",
    "\n",
    "        #which additional bins shall we use to measure metrics in?\n",
    "        metric_bins: [MAG_DETMODEL_I: '[ 17.5, 19, 22, 25]']\n",
    "        \n",
    "        #which pdf/ distribution comparison metrics should we measure \n",
    "        metrics: [bh_photo_z_validation.kstest, bh_photo_z_validation.npoisson, bh_photo_z_validation.log_loss]\n",
    "        tolerance: [0.7, 20]\n",
    "        #shall we use weights when calculating metrics, if so specify here.\n",
    "        weights: WEIGHTS\n",
    "===== end YaML file =======\n",
    "\n",
    "You see that we have now defined a set of tests for both point predictions, and pdfs.\n",
    "\n",
    "\n",
    "<h3>Loading the tests</h3>\n",
    "<p>We load the tests like this!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This test is called: photoz-wg\n",
      "\n",
      "{'pdf': {'stacks': {'metrics': ['bh_photo_z_validation.ks_test', 'bh_photo_z_validation.npoisson', 'bh_photo_z_validation.log_loss'], 'truths': 'Z_SPEC', 'weights': 'weights_valid', 'bins': None}}, 'test_name': 'photoz-wg', 'point': {'metrics': ['numpy.median', 'bh_photo_z_validation.sigma_68', 'bh_photo_z_validation.outlier_fraction'], 'weights': 'weights_valid', 'error_function': ['bh_photo_z_validation.bootstrap_mean_error'], 'tolerance': None, 'truths': 'Z_SPEC', 'predictions': ['MEDIAN_Z', 'MODE_Z', 'MEAN_Z'], 'bins': [{'MEDIAN_Z': '[0.0, 0.3, 0.6, 0.9, 1.3, 2.0]'}]}}\n",
      "\n",
      "\n",
      "Example of extracting a statistic to measure\n",
      "\n",
      "bh_photo_z_validation.sigma_68\n",
      "\n",
      "Example of which error we can assign to this metric\n",
      "\n",
      "bh_photo_z_validation.bootstrap_mean_error\n",
      "\n",
      "This means look in bh_photo_z_validation.py to see this function. You can add your own error function too!\n"
     ]
    }
   ],
   "source": [
    "testYamlPath = path_bh + 'testConfig/photoz.yaml'\n",
    "testConfig = yaml.load(open(testYamlPath, 'r'))\n",
    "\n",
    "#check that the test is valid. This is die if it's not.\n",
    "isTestValid = pval.valid_tests(testConfig)\n",
    "\n",
    "#the YaML file is parsed nicely to a python dictionary!\n",
    "print \"This test is called: \" + testConfig['test_name'] + '\\n'\n",
    "print testConfig\n",
    "print \"\\n\\nExample of extracting a statistic to measure\\n\"\n",
    "print testConfig['point']['metrics'][1]\n",
    "print \"\\nExample of which error we can assign to this metric\\n\"\n",
    "print testConfig['point']['error_function'][0]\n",
    "\n",
    "print \"\\nThis means look in bh_photo_z_validation.py to see this function. You can add your own error function too!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>YaML corrupt?</h3>\n",
    "<p>If your YaML file doesn't work, copy and check it here: http://yaml-online-parser.appspot.com\n",
    "\n",
    "<h3>Running a test for point predictions</h3>\n",
    "<p>Now we can play some testing magic. One thing to note, is they Python is <b>friggin</b> awesome. Remember we wrote the metric like this: 'bh_photo_z_validation.sigma_68', well we can turn this into an executable using a function found in bh_photo_z_validation.py called get_function(). We'll see this in the below.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "200 BootStrap errors calculated in  in 0.000016 secs for 10000 galaxies\n",
      " \n",
      "\n",
      "using file: ../validation/tests/data/validPointPrediction.fits\n",
      "point prediction: MEDIAN_Z\n",
      "we measure the statistic: median\n",
      "and get the value: 0.194681108167 [or error and mean from bootstrap] {'sigma': 0.0044039741907863856, 'mean': 0.19431982633023295}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3cae91eed4c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"we measure the statistic: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meachMetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"and get the value: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" [or error and mean from bootstrap]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mmetric_value\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mpointTestConfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tolerance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"This bettter than expected!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "pointTestConfig = testConfig['point']\n",
    "for eachPointPrediction in pointTestConfig['predictions']:\n",
    "    #let's calcalate the redshift scaled residuals Deltaz = (z - photz) / (1 + z)\n",
    "    deltaz_1pz = pval.delta_z_1pz(dataFrame[pointTestConfig['truths']], dataFrame[eachPointPrediction])\n",
    "    \n",
    "    #are we adding weights to each galaxy prediction?\n",
    "    weights = dataFrame[pointTestConfig['weights']]\n",
    "    \n",
    "    #now let's calculate the value of each chosen metric\n",
    "    for i, eachMetric in enumerate(pointTestConfig['metrics']):\n",
    "        \n",
    "        #amazing metric string to function conversion. \n",
    "        #Add your own functions by putting them in bh_photo_z_validation.py\n",
    "        metric_function = pval.get_function(eachMetric)\n",
    "        metric_value =  metric_function(deltaz_1pz)\n",
    "        \n",
    "        t1 = time.time()\n",
    "        #no add our generic error function, we'll need weights for this\n",
    "        error_function = pval.get_function(pointTestConfig['error_function'][0])\n",
    "        print \"\\n200 BootStrap errors calculated in  in %0.6f secs for %0.1d galaxies\" % (time.time() - t1, len(dataFrame))\n",
    "        \n",
    "        error_value = error_function(deltaz_1pz, weights, metric_function)\n",
    "        \n",
    "        print \" \\n\"\n",
    "        print \"using file: \" + pointPredictionFitsPath \n",
    "        print \"point prediction: \" + eachPointPrediction \n",
    "        print \"we measure the statistic: \" + eachMetric.split('.')[-1] \n",
    "        print \"and get the value: \" + str(metric_value) + \" [or error and mean from bootstrap]\", error_value\n",
    "        if metric_value < pointTestConfig['tolerance']:\n",
    "            print \"This bettter than expected!\"\n",
    "        else:\n",
    "            print \"This is worse than expected!\"\n",
    "\n",
    "# All the results of these tests will be printed to the screen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pointTestConfig, pointTestConfig['weights'],dataFrame.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>pdf tests</h2>\n",
    "<p>Now let's turn our attention to the pdf tests. Let's load them and look at them.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdfTestConfig = testConfig['pdf']\n",
    "print pdfTestConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Load pdf data</h3>\n",
    "<p>Lets load some data, and check that the data file is valid, and contains the columns we will be using.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdfPredictionFitsPath = path_bh + 'tests/data/validHDF.hdf5'\n",
    "\n",
    "require_cols = ['Z_SPEC', 'COADD_OBJECTS_ID', 'MAG_DETMODEL_I']\n",
    "\n",
    "t1 = time.time()      \n",
    "okay, dataFrame = pval.valid_file(pdfPredictionFitsPath, require_cols)\n",
    "print \"\\npdf file loaded in %0.4f secs for %0.1d galaxies\" % (time.time() - t1, len(dataFrame))\n",
    "  \n",
    "if okay is False:\n",
    "    print \"the file is not in the correct format\"\n",
    "    print \"erorr message: \" + dataFrame\n",
    "#print dataFrame[0:1], 'lenght', len(dataFrame)\n",
    "print dataFrame.info()\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>pdf array</h3>\n",
    "Let's extract the pdfs as an N-darray of shape (ngals, nbins).\n",
    "\n",
    "We also have access to the bin edges using the pdf_key valus<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zcols = [c for c in dataFrame.keys() if 'pdf_' in c]\n",
    "print \"pdf bin cols\", zcols[0:3]\n",
    "zbins = np.array([float(c.split('f_')[-1]) for c in zcols])\n",
    "print \"pdf bin centers\", zbins[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdfs = dataFrame[zcols].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Applying some tests</h3>\n",
    "Let's apply some test. First let's stack all pdfs and plot them against the Z_SPEC dist. (note, this is all random data, so don't expect a correlation!). We'll use tools found in bh_photo_z_validation.py to do all the heavy lifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = time.time()          \n",
    "#stack all pdfs across all gals\n",
    "stackedPdf = pval.stackpdfs(pdfs)\n",
    "\n",
    "#normalise this stack\n",
    "normStackedPdf = pval.normalisepdfs(stackedPdf, zbins)\n",
    "\n",
    "#make df for Z_SPEC #numpy.histogram needs the final bin edge\n",
    "dndz = np.histogram(dataFrame['Z_SPEC'], bins=np.append(zbins,2))[0]*1.0\n",
    "z_pdf = pval.normalisepdfs(dndz, zbins)\n",
    "\n",
    "print \"\\n All computations took %0.4f secs for %0.1d galaxies\" % (time.time() - t1, len(pdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(zbins, z_pdf, label='z_pdf (random)', linewidth=1, alpha=0.7)\n",
    "plt.plot(zbins, normStackedPdf, label='normStackedPdf (random)', linewidth=2, alpha=0.7)\n",
    "plt.title('Using random test data')\n",
    "plt.xlabel('redshift')\n",
    "plt.ylabel('pdf')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fake data</h3>\n",
    "<p> Do note we are just using some meaningless, random, made up data.</p>\n",
    "<p>Let's continue with the tutorial and perform some other comparisons on these (meaningless) distributions!</p>\n",
    "<p>Let's find how much of each galaxy's pdf sits within some bin.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "pdf_in_int = pval.integrate_dist_bin(pdfs,zbins, 0.1, 0.3)\n",
    "print \"\\n All computations took %0.4f secs for %0.1d galaxies\" % (time.time() - t1, len(pdfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now, let's plot a few of those with lots of pdf in the bin</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = pdf_in_int > 0.95\n",
    "for j in range(4):\n",
    "    plt.plot(zbins, pdfs[i][j])\n",
    "plt.plot([0.1,0.1], [0,60], '--b')\n",
    "plt.plot([0.3,0.3], [0,60], '--b')\n",
    "plt.xlabel('z')\n",
    "plt.xlim(0,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and plot a few with only a little weight in the bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = (pdf_in_int > 0.3) * (pdf_in_int < 0.31)\n",
    "for j in range(4):\n",
    "    plt.plot(zbins, pdfs[i][j])\n",
    "plt.plot([0.1,0.1], [0,10], '--b')\n",
    "plt.plot([0.3,0.3], [0,10], '--b')\n",
    "plt.xlabel('z')\n",
    "plt.xlim(0.08,0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Bordoloi pdf test</h3>\n",
    "<p>One test that we perform is the Bordoloi et al 2012 http://arxiv.org/pdf/1201.0995.pdf test. The idea is to make a cumulative pdf for each galaxy, and then calculate the y-axis value that corresponds to the x-axis value of the spectroscopic (or other) redshift.\n",
    "<br>\n",
    "This statistics, averaged over many galaxies, should have a flat distribution in y-axis values (between 0-1). To check how flat it is, we can us the gini criteria! [https://en.wikipedia.org/wiki/Gini_coefficient]\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "Z_SPEC = dataFrame['Z_SPEC']\n",
    "\n",
    "nomalisepdfs = pval.normalisepdfs(pdfs, zbins)\n",
    "y_axis_vals = pval.cumaltive_to_point(nomalisepdfs, zbins, Z_SPEC)\n",
    "\n",
    "#what do these look like?\n",
    "#now let's make a histogram of this distribution\n",
    "N=100\n",
    "_ = plt.hist(y_axis_vals,bins=N)\n",
    "plt.ylim(0,100)\n",
    "\n",
    "\n",
    "#finally, let's measure how far away this is from a flat distribution; with gini = 0.0\n",
    "gini_val = pval.gini(np.histogram(y_axis_vals, bins=N)[0] * 1.0)\n",
    "print gini_val\n",
    "print \"this is quite flat! let's check it out\"\n",
    "cum = np.cumsum(np.histogram(y_axis_vals, bins=N)[0])\n",
    "cum = cum / float(cum[-1])\n",
    "f = plt.figure()\n",
    "plt.plot(np.arange(len(cum)), cum, label= 'this dist. gini= %0.3f' % gini_val)\n",
    "plt.plot(np.arange(len(cum)), np.arange(len(cum))*1.0/len(cum), label='perfect gini=0')\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('z bins')\n",
    "plt.ylabel('c-pdf value')\n",
    "print \"\\n All computations (including plots) took %0.4f secs for %0.1d galaxies\" % (time.time() - t1, len(pdfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Boot strap error on these stats</h3>\n",
    "<p>We can again calculate a boot strap error, using a wrapper written in bh_photo_z_validation.py called <code>bootstrap_mean_error_pdf_point()</code>. You are welcome to write your own, and call is as before (yourFileName.yourFunctionName). Let's first get hold of the weights, and then run the internal functions. These calculations are a little slow ~30secs on my computer (for 10k galaxies) </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "weights = dataFrame['WEIGHT'] #also accesable through pdfTestConfig['individual']['weights']\n",
    "meanErr = pval.bootstrap_mean_error_pdf_point(nomalisepdfs, zbins, Z_SPEC, weights, pval.Bordoloi_pdf_test)\n",
    "print \"mean and error on \", len(dataFrame), \" pdfs with 200 BootStrap resamples\", meanErr\n",
    "print \"\\n All computations took %0.4f secs for %0.1d galaxies\" % (time.time() - t1, len(pdfs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Compare dn/dz distributions</h3>\n",
    "<p>We also compare dndz distributions, using either built in metrics, or as before, you can write your own. These metrics take the un-normalised distributions (and should normalise if required). Let's start with an exmaple:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#which tests do we want to perform?\n",
    "print \n",
    "test = testConfig['pdf']['stacks']\n",
    "print \"\\nThis means that we will use \" + test['truths'] + \" as the truth column, and form dNdz bins like\"\n",
    "print test['truth_bins'][0][test['truths']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's explore this first test, bh_photo_z_validation.npoisson</p> <p>Let's first make the dNdz of the \"truth\" distribution, we will be comparing to. Be careful! If this have bins with 0 in them, many statistical routines will fail!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "truth_col = test['truth_bins'][0].keys()[0]\n",
    "\n",
    "truth_bins_edges = eval(test['truth_bins'][0][truth_col])\n",
    "truths = np.array(dataFrame[truth_col])\n",
    "truth_dist = np.histogram(truths, bins=truth_bins_edges)[0]*1.0\n",
    "\n",
    "#let's be careful about the bin centers: they are not! the center of the bin, but the average value in the bin\n",
    "truth_bins_centers = stats.binned_statistic(truths, truths, bins=truth_bins_edges, statistic=np.mean).statistic\n",
    "\n",
    "plt.plot(truth_bins_centers, truth_dist, label='Truth: ' + truth_col)\n",
    "plt.legend(loc=1)\n",
    "plt.ylabel('Num/bin')\n",
    "plt.xlabel(truth_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now let's do the same for the pdfs, and perfrom the tests. All of these messy details are hidden from the user inby calling the photoz_metric.py script.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's get pdf_ column headings, and extract the value of the bin edge:\n",
    "zcols = [c for c in dataFrame.keys() if 'pdf_' in c]\n",
    "pdf_z_edge = np.array([float(c.split('f_')[-1]) for c in zcols])\n",
    "pdf_z_center = pdf_z_edge + np.append((pdf_z_edge[1:] - pdf_z_edge[0: -1]) / 2.0, (pdf_z_edge[-1] - pdf_z_edge[-2]) / 2.0)\n",
    "\n",
    "#the pdf's are store here for each galaxy\n",
    "pdf = np.array(dataFrame[zcols])\n",
    "\n",
    "#and we can stack them using this code\n",
    "stacked_pdf = pval.stackpdfs(pdf)\n",
    "\n",
    "#let's plot the result!\n",
    "\n",
    "plt.plot(pdf_z_center, stacked_pdf, label='Stacked pdf')\n",
    "plt.plot(truth_bins_centers, truth_dist, label='Truth: ' + truth_col)\n",
    "plt.legend(loc=1)\n",
    "plt.ylabel('Num/bin')\n",
    "plt.xlabel(truth_col)\n",
    "plt.title('Fake data!')\n",
    "\n",
    "#beware that we may have truth's at a different binning resolution than the pdfs. We need to combine the pdfs to cope with this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we are ready to perform the comparison test! Just like before, we can *also* make the test by binning in a choice of features. See the yaml file for more details.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stckd_pdfs_at_trth_cntrs = pval.interpolate_dist(stacked_pdf, pdf_z_center, truth_bins_centers)\n",
    "\n",
    "print 'Npoisson value', pval.npoisson(truth_dist, stckd_pdfs_at_trth_cntrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Results files</h2>\n",
    "<p>We write the results of the validation script (recall it's called like) </p>\n",
    "\n",
    "<code>%>photoz_metrics.py data/PointPredictions1.fits </code>\n",
    "\n",
    "<p>Using a results File Prefix which you may have set in the testConfiguration Yaml file.\n",
    "\n",
    "<p>Into a lovely YaML file for human easy reading, and also as a pickle file. This means we can pass the outputs into the plotting tool. An example of that is (or will be) in this notebook directory soon! </p>\n",
    "\n",
    "<h3>Output</h3>\n",
    "\n",
    "<p>Example of the output is below:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#in this example we don't have resultsFilePrefix. \n",
    "#print \"test prefix\", testConfig['resultsFilePrefix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#output file name:\n",
    "pointResults = 'point_.p'\n",
    "pointResults = 'point_.yaml'\n",
    "pdfResults = 'pdf_.p'\n",
    "pdfResults = 'pdf_.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The results are formatted to look like this: (in e.g. points_.yaml), and can be read using <code>python pickle</code> or <code>yaml.load()</code> as before </p>\n",
    "\n",
    "<code>\n",
    "tests/data/validPointPrediction.fits:\n",
    "  0:\n",
    "    Z_MC:\n",
    "      bh_photo_z_validation.sigma_68:\n",
    "        delta_z:\n",
    "          VALUE: 0.7035013244853514\n",
    "          bins:\n",
    "            Z_MC:\n",
    "              BIN_CENTERS:\n",
    "              - 0.2593094256649402\n",
    "              - 0.7587431412716394\n",
    "              - 1.4909100996939546\n",
    "              BIN_CENTERS_MEAN_BS:\n",
    "              - 0.2617489006521707\n",
    "              - 0.759508863364966\n",
    "              - 1.4927365449936938\n",
    "              BIN_CENTERS_SIGMA_BS:\n",
    "              - 0.008674035664842377\n",
    "              - 0.009549516216673822\n",
    "              - 0.013034528528585611\n",
    "              MEAN_BS:\n",
    "              - 0.334349762217648\n",
    "              - 0.34009724283466963\n",
    "              - 0.40911843435271694\n",
    "              SIGMA_BS:\n",
    "              - 0.019772862096479635\n",
    "              - 0.02627186735715245\n",
    "              - 0.016312035676113744\n",
    "              VALUE:\n",
    "              - 0.3283407192008526\n",
    "              - 0.32228114188030743\n",
    "              - 0.41986501332174475\n",
    "        diff_1pz:\n",
    "          VALUE: 0.2840504374875773\n",
    "          bins:\n",
    "            Z_MC:\n",
    "              BIN_CENTERS:\n",
    "              - 0.2593094256649402\n",
    "              - 0.7587431412716394\n",
    "              - 1.4909100996939546\n",
    "              BIN_CENTERS_MEAN_BS:\n",
    "              - 0.26151256908410203\n",
    "              - 0.7593840546088471\n",
    "              - 1.492391911786944\n",
    "              BIN_CENTERS_SIGMA_BS:\n",
    "              - 0.00886473342482512\n",
    "              - 0.009578223372819055\n",
    "              - 0.013132219172208937\n",
    "              MEAN_BS:\n",
    "              - 0.08723858288599118\n",
    "              - 0.11446066934486837\n",
    "              - 0.17185082125162893\n",
    "              SIGMA_BS:\n",
    "              - 0.0060519403768464885\n",
    "              - 0.008752563717080358\n",
    "              - 0.006043700892506574\n",
    "              VALUE:\n",
    "              - 0.08452927318731282\n",
    "              - 0.10696974619042877\n",
    "              - 0.17107178338923312\n",
    "\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for_WL = pval.weighted_nz_distributions(dataFrame, binning=zbins, weights=None, tomo_bins=[0,0.2,0.5], z_phot= dataFrame.Z_SPEC.values , n_resample=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_mean = pval.mean(dataFrame, binning=zbins, weights=None, metric='mode', tomo_bins=[0,0.2,0.5, 1.0, 2.0], z_phot= dataFrame.Z_SPEC.values , n_resample=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/Christopher_old/Desktop/RF_S82_point_estimates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['MEAN_Z'] = data.Z_PHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('/Users/Christopher_old/Desktop/RF_S82_point_estimates.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
