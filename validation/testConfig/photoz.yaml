#these are the point prediction tests
point:
    #which photo-z predictions do we want to test
    predictions: [MODE_Z, MEAN_Z, Z_MC]
    
    #what is the true redshift that we will compare with?
    truths: Z_SPEC
    
    #shall we use weights when calculating metrics, if so specify here.
    weights: WEIGHTS

    #what metrics do we want to measure. "numpy.std" is the standard deviation from numpy
    # and "bh_photo_z_validation.sigma_68" is the sigma_68 metric found in the bh_photo_z_validation.py file
    metrics: [numpy.std, numpy.median, bh_photo_z_validation.sigma_68, bh_photo_z_validation.outlier_fraction]
    
    #do we want to assign an accetable tolerance to each of these tests?
    tolerance: [0.4, 0.001, 0.02, 5]
    
    #Finally do we want to also measure the metrics in some "bins". 
    #we define the column_name: 'string of bins / string of function that makes bins'
    bins: [MAG_DETMODEL_I: '[ 17.5, 19, 22, 25]', MODE_Z: 'numpy.linspace(0, 2, 4)']

    #Should we calculate errors on each metric? if yes state how
    #you can include as many different error functions as you like.
    error_function: [bh_photo_z_validation.bootstrap_mean_error]
    

#these are the pdf tests
pdf: 
    #what is the true z of distribution of z's
    truths: Z_SPEC

    #in this example I don't want to do any binning.
    bins:
    
    #I just want to measure these pdf comparison metrics
    metrics: [bh_photo_z_validation.kstest, bh_photo_z_validation.npoisson, bh_photo_z_validation.log_loss]
    
    #shall we use weights when calculating metrics, if so specify here.
    weights: WEIGHTS

    #And I don't care about tolerances.
    tolerance:
